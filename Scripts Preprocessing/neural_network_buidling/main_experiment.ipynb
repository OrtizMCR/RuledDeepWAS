{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main experiment\n",
    "\n",
    "Train a neural network to classify between three different classes of genomic sequences:\n",
    "* Sequences from the reference genome\n",
    "* Altered sequences based on information from Ensembl Variation build 110\n",
    "* Altered sequences based on information from the GWAS Catalog\n",
    "\n",
    "The last two represent sequences that include SNV without and with gwas-association, respectively.\n",
    "\n",
    "The steps to run the experiment are:\n",
    "1. Load the dataset from each chromosome, containing the sequences and their corresponding label.\n",
    "2. Turn these datasets into an unique big general dataset\n",
    "3. Apply the tokenization and pass into DataLoaders\n",
    "4. Instantiate a DNABERT model for sequence classification between 3 classes\n",
    "5. Train the neural network\n",
    "6. Evaluate its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary modules and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases_path = \"/mnt/SILVER_DATA/Moy/Moys_dataset/\"\n",
    "# GWAS Catalog path\n",
    "gwas_catalog_path = os.path.join(databases_path, \"GWAS_Catalog_DATA/gwas_catalog_v1.0.2-associations_e110_r2023-09-25.tsv\")\n",
    "# Ensembl Variation path\n",
    "ensembl_path = os.path.join(databases_path, \"Ensembl/Variation/110/\")\n",
    "# Chromosomes' data path\n",
    "chromosomes_path = os.path.join(ensembl_path, \"chromosomes_data/\")\n",
    "# Reference genome path\n",
    "ref_genome_path = os.path.join(databases_path,\"Reference_Genome/GRCh38p14/Ensembl/Homo_sapiens_GRCh38_dna_primary_assembly.fa\")\n",
    "# GWAS Associated bed and sequences path\n",
    "gwas_associated_bed_path = os.path.join(databases_path, \"Ensembl/Variation/110/gwas_associated_sequences/beds\")\n",
    "gwas_associated_seq_path = os.path.join(databases_path, \"Ensembl/Variation/110/gwas_associated_sequences/ref_sequences\")\n",
    "rand_bed_path = os.path.join(databases_path, \"Ensembl/Variation/110/random_sequences/beds\")\n",
    "rand_seq_path = os.path.join(databases_path, \"Ensembl/Variation/110/random_sequences/ref_sequences\")\n",
    "# Datasets path\n",
    "dataset_path = os.path.join(ensembl_path, \"chromosome_datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18614/1446307973.py:5: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  main_dataset = pd.concat([main_dataset, chr_dataset])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1120243, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromosomes = ['2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y']\n",
    "main_dataset = pd.read_csv(os.path.join(ensembl_path, 'to_dataloaders/chr1_dataset.csv'), index_col=0)\n",
    "for chromosome in chromosomes:\n",
    "    chr_dataset = pd.read_csv(os.path.join(ensembl_path, 'to_dataloaders/chr{}_dataset.csv'.format(chromosome)), index_col=0)\n",
    "    main_dataset = pd.concat([main_dataset, chr_dataset])\n",
    "\n",
    "# Uncomment if CustomTrainer is used\n",
    "#main_dataset['label'] += 1\n",
    "\n",
    "main_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    485478\n",
       "2    367305\n",
       "1    267460\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch dataset objects with the HuggingFace module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['seq', 'label', '__index_level_0__'],\n",
       "        num_rows: 896194\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['seq', 'label', '__index_level_0__'],\n",
       "        num_rows: 224049\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# Using the HuggingFace framework to create the Datasets and DataLoaders and split the data into train and test set\n",
    "hf_dataset = Dataset.from_pandas(main_dataset).train_test_split(test_size=0.2)\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sequences\n",
    "Expensive process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7b3a5617154245bfd2d6e78ea651e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/896194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7900598efc84bebbce15c9d33342fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/224049 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['seq', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 896194\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['seq', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 224049\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"seq\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = hf_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unnecessary elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'test': ['label', 'input_ids', 'token_type_ids', 'attention_mask']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"seq\", \"__index_level_0__\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 64 #move to 16\n",
    "train_set = DataLoader(tokenized_datasets[\"train\"], batch_size=batch_sz, shuffle=True, collate_fn=data_collator)\n",
    "test_set = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_sz, shuffle=True, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first batch to check everything is alright:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([64, 31]),\n",
       " 'token_type_ids': torch.Size([64, 31]),\n",
       " 'attention_mask': torch.Size([64, 31]),\n",
       " 'labels': torch.Size([64])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_set:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and instantiate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if cuda is available and define it as the processing device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'classifier.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'classifier.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with 3 labels: [ref_sequences, alt_sequences, gwas_alt_sequences]\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"zhihan1996/DNABERT-2-117M\", num_labels=3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"zhihan1996/DNABERT-2-117M\",\n",
       "  \"alibi_starting_size\": 512,\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"zhihan1996/DNABERT-2-117M--configuration_bert.BertConfig\",\n",
       "    \"AutoModel\": \"zhihan1996/DNABERT-2-117M--bert_layers.BertModel\",\n",
       "    \"AutoModelForMaskedLM\": \"zhihan1996/DNABERT-2-117M--bert_layers.BertForMaskedLM\",\n",
       "    \"AutoModelForSequenceClassification\": \"zhihan1996/DNABERT-2-117M--bert_layers.BertForSequenceClassification\"\n",
       "  },\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 4096\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the model works by passing the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0604, device='cuda:0', grad_fn=<NllLossBackward0>) torch.Size([64, 3])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch.to(device))\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "Define the optimizer, scheduler, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42012\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 1e-8\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=weight_decay)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_set)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loop\n",
    "Training loop for classification tasks with unbalanced datasets.\n",
    "> Change the label column in dataset adding 1 to the whole column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs[\"labels\"]\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.0, 1.0, 2.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(model=model, train_dataset=train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    # Implement your accuracy computation logic here\n",
    "    # This could involve comparing predicted labels to actual labels\n",
    "    # and computing the accuracy metric.\n",
    "    # For example:\n",
    "    correct_predictions = (predictions == labels).sum().item()\n",
    "    total_predictions = len(labels)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dc97008c1f4c5f9af2a3315faff468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-----------------------\n",
      "----------------\n",
      "Loss: 1.1958754062652588\n",
      "\n",
      "Accuracy: 0.421875\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6148900985717773\n",
      "\n",
      "Accuracy: 0.28125\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6092537641525269\n",
      "\n",
      "Accuracy: 0.34375\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5392376780509949\n",
      "\n",
      "Accuracy: 0.390625\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.7727342247962952\n",
      "\n",
      "Accuracy: 0.171875\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6831099987030029\n",
      "\n",
      "Accuracy: 0.234375\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6409198045730591\n",
      "\n",
      "Accuracy: 0.359375\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5746217966079712\n",
      "\n",
      "Accuracy: 0.3125\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6539207100868225\n",
      "\n",
      "Accuracy: 0.25\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5485142469406128\n",
      "\n",
      "Accuracy: 0.328125\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.587041437625885\n",
      "\n",
      "Accuracy: 0.40625\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6103031039237976\n",
      "\n",
      "Accuracy: 0.234375\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.4831559956073761\n",
      "\n",
      "Accuracy: 0.46875\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6757141947746277\n",
      "\n",
      "Accuracy: 0.15625\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5220099091529846\n",
      "\n",
      "Accuracy: 0.328125\n",
      "----------------\n",
      "-------- \n",
      " Epoch 0: Average Loss: 0.5813461542129517\n",
      "Epoch 2\n",
      "-----------------------\n",
      "----------------\n",
      "Loss: 0.6412397027015686\n",
      "\n",
      "Accuracy: 0.296875\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5641384124755859\n",
      "\n",
      "Accuracy: 0.328125\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5300788879394531\n",
      "\n",
      "Accuracy: 0.375\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5738858580589294\n",
      "\n",
      "Accuracy: 0.28125\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5254380702972412\n",
      "\n",
      "Accuracy: 0.390625\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5034897923469543\n",
      "\n",
      "Accuracy: 0.34375\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5391408801078796\n",
      "\n",
      "Accuracy: 0.40625\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5451075434684753\n",
      "\n",
      "Accuracy: 0.359375\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6064088344573975\n",
      "\n",
      "Accuracy: 0.328125\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6811520457267761\n",
      "\n",
      "Accuracy: 0.234375\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6300005912780762\n",
      "\n",
      "Accuracy: 0.265625\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6147831082344055\n",
      "\n",
      "Accuracy: 0.28125\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.5534239411354065\n",
      "\n",
      "Accuracy: 0.375\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.6852788329124451\n",
      "\n",
      "Accuracy: 0.25\n",
      "----------------\n",
      "----------------\n",
      "Loss: 0.4624389410018921\n",
      "\n",
      "Accuracy: 0.421875\n",
      "----------------\n",
      "-------- \n",
      " Epoch 1: Average Loss: nan\n",
      "Epoch 3\n",
      "-----------------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.4375\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.453125\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.46875\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.421875\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.296875\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.375\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.421875\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.390625\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.453125\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.3125\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.34375\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.515625\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.4375\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.46875\n",
      "----------------\n",
      "----------------\n",
      "Loss: nan\n",
      "\n",
      "Accuracy: 0.328125\n",
      "----------------\n",
      "-------- \n",
      " Epoch 2: Average Loss: nan\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3  # Number of epochs with no improvement after which to stop\n",
    "best_val_loss = float('inf')\n",
    "counter = 0  # Counter for tracking epochs without improvement\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    print(f\"Epoch {epoch+1}\\n-----------------------\")\n",
    "\n",
    "    for i, batch in enumerate(train_set):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        #outputs = model(**batch)\n",
    "        #loss = outputs.loss\n",
    "        loss, outputs = trainer.compute_loss(model, batch, return_outputs=True)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        accuracy = compute_accuracy(predictions, batch['labels'])\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "             print(\"----------------\\nLoss: {}\".format(loss))\n",
    "             print(\"\\nAccuracy: {}\\n----------------\".format(accuracy))\n",
    "             \n",
    "\n",
    "    average_loss = total_loss / len(train_set)\n",
    "    average_accuracy = total_accuracy / len(train_set)\n",
    "    print(\"-\"*8,\"\\n\",\"Epoch {}: Average Loss: {}\".format(epoch+1, average_loss))\n",
    "\n",
    "    # Early stopping check    \n",
    "    if average_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), \"/home/labsilver/Moy/models/dnabert.pth\")\n",
    "        best_val_loss = average_loss\n",
    "        counter = 0  # Reset counter if there's improvement\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break  # Stop training\n",
    "    \n",
    "    train_losses.append(average_loss)\n",
    "    train_accuracies.append(average_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0103452a6c7047859723bb2a254adbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/367590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-----------------------\n",
      "Loss: 1.0637295246124268\n",
      "Loss: 1.1702585220336914\n",
      "Loss: 0.9242050051689148\n",
      "Loss: 1.2005733251571655\n",
      "Loss: 1.1292930841445923\n",
      "Loss: 1.1586413383483887\n",
      "Loss: 1.1084024906158447\n",
      "Loss: 1.1413960456848145\n",
      "Loss: 1.1064064502716064\n",
      "Loss: 1.0894410610198975\n",
      "Loss: 1.0423803329467773\n",
      "Loss: 1.0218384265899658\n",
      "Loss: 1.2721714973449707\n",
      "Loss: 1.0099130868911743\n",
      "Loss: 1.134857177734375\n",
      "Loss: 1.072811484336853\n",
      "Loss: 1.087422251701355\n",
      "Loss: 1.113666296005249\n",
      "Loss: 1.1467797756195068\n",
      "Loss: 1.0663812160491943\n",
      "Loss: 1.106511116027832\n",
      "Loss: 1.0830333232879639\n",
      "Loss: 1.0738954544067383\n",
      "Loss: 0.9863682985305786\n",
      "Loss: 1.2351248264312744\n",
      "Loss: 1.1349526643753052\n",
      "Loss: 1.0711053609848022\n",
      "Loss: 1.025098443031311\n",
      "Loss: 1.0234369039535522\n",
      "Loss: 1.0322034358978271\n",
      "Loss: 1.0628880262374878\n",
      "Loss: 1.1505389213562012\n",
      "Loss: 1.1116677522659302\n",
      "Loss: 1.0789353847503662\n",
      "Loss: 1.1160171031951904\n",
      "Loss: 1.0437536239624023\n",
      "Loss: 1.0602350234985352\n",
      "Loss: 1.0495884418487549\n",
      "Loss: 1.0202350616455078\n",
      "Loss: 0.9933459758758545\n",
      "Loss: 1.0416841506958008\n",
      "Loss: 1.031018614768982\n",
      "Loss: 1.025987982749939\n",
      "Loss: 0.9962440729141235\n",
      "Loss: 1.0622133016586304\n",
      "Loss: 1.10015869140625\n",
      "Loss: 1.0882149934768677\n",
      "Loss: 1.0308079719543457\n",
      "Loss: 1.054574728012085\n",
      "Loss: 1.0348780155181885\n",
      "Loss: 1.1058984994888306\n",
      "Loss: 1.0820826292037964\n",
      "Loss: 1.0964158773422241\n",
      "Loss: 1.0956727266311646\n",
      "Loss: 1.1017478704452515\n",
      "Loss: 1.1190028190612793\n",
      "Loss: 1.140132188796997\n",
      "Loss: 1.1061286926269531\n",
      "Loss: 1.0212557315826416\n",
      "Loss: 1.1132211685180664\n",
      "Loss: 1.1432260274887085\n",
      "Loss: 1.0841002464294434\n",
      "Loss: 1.0886081457138062\n",
      "Loss: 1.0345622301101685\n",
      "Loss: 1.0864356756210327\n",
      "Loss: 1.0547868013381958\n",
      "Loss: 1.0741846561431885\n",
      "Loss: 1.087204098701477\n",
      "Loss: 1.0772223472595215\n",
      "Loss: 1.0820611715316772\n",
      "Loss: 1.0558522939682007\n",
      "Loss: 1.091928482055664\n",
      "Loss: 1.0512300729751587\n",
      "Loss: 1.119580626487732\n",
      "Loss: 1.0886883735656738\n",
      "Loss: 1.1235123872756958\n",
      "Loss: 1.1594127416610718\n",
      "Loss: 1.0534236431121826\n",
      "Loss: 1.1074323654174805\n",
      "Loss: 1.1469461917877197\n",
      "Loss: 1.1262428760528564\n",
      "Loss: 0.9903174042701721\n",
      "Loss: 1.1960480213165283\n",
      "Loss: 1.0603543519973755\n",
      "Loss: 1.094693660736084\n",
      "Loss: 1.076967477798462\n",
      "Loss: 1.061993956565857\n",
      "Loss: 1.0337250232696533\n",
      "Loss: 1.0546644926071167\n",
      "Loss: 1.170207142829895\n",
      "Loss: 1.1516854763031006\n",
      "Loss: 1.0380144119262695\n",
      "Loss: 1.07655930519104\n",
      "Loss: 0.9973966479301453\n",
      "Loss: 1.1317689418792725\n",
      "Loss: 0.9786739349365234\n",
      "Loss: 1.0357694625854492\n",
      "Loss: 1.0861207246780396\n",
      "Loss: 1.0123016834259033\n",
      "Loss: 1.0971847772598267\n",
      "Loss: 1.11348557472229\n",
      "Loss: 1.0522711277008057\n",
      "Loss: 1.0701650381088257\n",
      "Loss: 1.086323857307434\n",
      "Loss: 1.0656503438949585\n",
      "Loss: 1.0370310544967651\n",
      "Loss: 1.143913984298706\n",
      "Loss: 1.1471316814422607\n",
      "Loss: 1.0761213302612305\n",
      "Loss: 1.103535771369934\n",
      "Loss: 1.0752941370010376\n",
      "Loss: 1.014051079750061\n",
      "Loss: 1.0363788604736328\n",
      "Loss: 1.0259158611297607\n",
      "Loss: 1.0301365852355957\n",
      "Loss: 1.043631911277771\n",
      "Loss: 1.0390750169754028\n",
      "Loss: 1.104393482208252\n",
      "Loss: 1.0478575229644775\n",
      "Loss: 1.09744393825531\n",
      "Loss: 1.174845814704895\n",
      "Loss: 1.1513859033584595\n",
      "Loss: 1.118599772453308\n",
      "Loss: 1.0633996725082397\n",
      "Loss: 1.0974552631378174\n",
      "Loss: 1.0939860343933105\n",
      "Loss: 1.1061404943466187\n",
      "Loss: 1.0884172916412354\n",
      "Loss: 1.0276259183883667\n",
      "Loss: 1.0557457208633423\n",
      "Loss: 1.0578886270523071\n",
      "Loss: 1.0899285078048706\n",
      "Loss: 1.0933837890625\n",
      "Loss: 1.074729084968567\n",
      "Loss: 1.0727821588516235\n",
      "Loss: 1.1118950843811035\n",
      "Loss: 0.973412811756134\n",
      "Loss: 1.1333398818969727\n",
      "Loss: 1.1875975131988525\n",
      "Loss: 1.0283375978469849\n",
      "Loss: 1.0943857431411743\n",
      "Loss: 1.0288105010986328\n",
      "Loss: 1.0793133974075317\n",
      "Loss: 1.0469852685928345\n",
      "Loss: 1.0200574398040771\n",
      "Loss: 1.0355937480926514\n",
      "Loss: 1.1001819372177124\n",
      "Loss: 1.0535403490066528\n",
      "Loss: 1.136059284210205\n",
      "Loss: 1.0655746459960938\n",
      "Loss: 1.0894479751586914\n",
      "Loss: 1.1054294109344482\n",
      "Loss: 0.9613870978355408\n",
      "Loss: 1.0802842378616333\n",
      "Loss: 1.0436066389083862\n",
      "Loss: 1.1218129396438599\n",
      "Loss: 1.01993727684021\n",
      "Loss: 1.1275821924209595\n",
      "Loss: 1.0271008014678955\n",
      "Loss: 1.1105453968048096\n",
      "Loss: 1.119279146194458\n",
      "Loss: 1.07052481174469\n",
      "Loss: 1.0803462266921997\n",
      "Loss: 1.0672684907913208\n",
      "Loss: 1.013973355293274\n",
      "Loss: 1.086032509803772\n",
      "Loss: 1.0401935577392578\n",
      "Loss: 1.099050521850586\n",
      "Loss: 1.0590832233428955\n",
      "Loss: 1.0302584171295166\n",
      "Loss: 1.0348947048187256\n",
      "Loss: 1.013272762298584\n",
      "Loss: 1.1003751754760742\n",
      "Loss: 1.0463030338287354\n",
      "Loss: 1.0563650131225586\n",
      "Loss: 1.1275734901428223\n",
      "Loss: 1.036334753036499\n",
      "Loss: 1.004960060119629\n",
      "Loss: 1.0455328226089478\n",
      "Loss: 1.1347671747207642\n",
      "Loss: 1.0376819372177124\n",
      "Loss: 1.055219292640686\n",
      "Loss: 1.1033852100372314\n",
      "Loss: 1.1078943014144897\n",
      "Loss: 1.0282071828842163\n",
      "Loss: 1.0511525869369507\n",
      "Loss: 1.0935338735580444\n",
      "Loss: 1.0053019523620605\n",
      "Loss: 1.0186959505081177\n",
      "Loss: 1.0652953386306763\n",
      "Loss: 1.082543969154358\n",
      "Loss: 1.0883371829986572\n",
      "Loss: 1.0100270509719849\n",
      "Loss: 1.1007267236709595\n",
      "Loss: 1.166739583015442\n",
      "Loss: 1.069021224975586\n",
      "Loss: 1.1129016876220703\n",
      "Loss: 1.05216646194458\n",
      "Loss: 1.0544089078903198\n",
      "Loss: 1.08456552028656\n",
      "Loss: 1.006371021270752\n",
      "Loss: 1.0615354776382446\n",
      "Loss: 1.0981791019439697\n",
      "Loss: 1.0766047239303589\n",
      "Loss: 1.0752867460250854\n",
      "Loss: 1.0604147911071777\n",
      "Loss: 1.0557078123092651\n",
      "Loss: 1.1399283409118652\n",
      "Loss: 1.054720401763916\n",
      "Loss: 1.016801357269287\n",
      "Loss: 1.0822221040725708\n",
      "Loss: 1.086964726448059\n",
      "Loss: 1.018480896949768\n",
      "Loss: 1.1487398147583008\n",
      "Loss: 1.0844779014587402\n",
      "Loss: 0.9922124743461609\n",
      "Loss: 1.103832721710205\n",
      "Loss: 1.1742883920669556\n",
      "Loss: 1.0923163890838623\n",
      "Loss: 1.0527503490447998\n",
      "Loss: 1.061026930809021\n",
      "Loss: 1.1066288948059082\n",
      "Loss: 1.1219724416732788\n",
      "Loss: 1.0291613340377808\n",
      "Loss: 1.0879429578781128\n",
      "Loss: 1.0114549398422241\n",
      "Loss: 1.0536590814590454\n",
      "Loss: 1.0639593601226807\n",
      "Loss: 1.00902259349823\n",
      "Loss: 1.0113375186920166\n",
      "Loss: 1.1411874294281006\n",
      "Loss: 1.0483869314193726\n",
      "Loss: 1.1070291996002197\n",
      "Loss: 1.1270372867584229\n",
      "Loss: 1.032426118850708\n",
      "Loss: 1.0609815120697021\n",
      "Loss: 1.0702900886535645\n",
      "Loss: 1.1035218238830566\n",
      "Loss: 1.0855169296264648\n",
      "Loss: 1.0804364681243896\n",
      "Loss: 1.0887885093688965\n",
      "Loss: 1.107749581336975\n",
      "Loss: 1.0743647813796997\n",
      "Loss: 1.026639461517334\n",
      "Loss: 1.073226809501648\n",
      "Loss: 1.0590497255325317\n",
      "-------- \n",
      " Epoch 0: Average Loss: 1.0759503841400146\n",
      "Epoch 2\n",
      "-----------------------\n",
      "Loss: 0.9864846467971802\n",
      "Loss: 1.0278277397155762\n",
      "Loss: 1.1010633707046509\n",
      "Loss: 1.037414312362671\n",
      "Loss: 1.1092941761016846\n",
      "Loss: 1.0702687501907349\n",
      "Loss: 1.0442646741867065\n",
      "Loss: 1.099876880645752\n",
      "Loss: 1.0534864664077759\n",
      "Loss: 1.0565828084945679\n",
      "Loss: 1.1260912418365479\n",
      "Loss: 1.1288617849349976\n",
      "Loss: 1.082363486289978\n",
      "Loss: 1.0676904916763306\n",
      "Loss: 1.0456149578094482\n",
      "Loss: 1.1318291425704956\n",
      "Loss: 1.1127694845199585\n",
      "Loss: 1.095705270767212\n",
      "Loss: 1.0610594749450684\n",
      "Loss: 1.0411872863769531\n",
      "Loss: 1.1460334062576294\n",
      "Loss: 1.0674338340759277\n",
      "Loss: 1.111389398574829\n",
      "Loss: 1.0703210830688477\n",
      "Loss: 1.0652284622192383\n",
      "Loss: 1.1038312911987305\n",
      "Loss: 1.043177604675293\n",
      "Loss: 1.1926956176757812\n",
      "Loss: 1.039743423461914\n",
      "Loss: 1.0688369274139404\n",
      "Loss: 1.1026852130889893\n",
      "Loss: 1.0582398176193237\n",
      "Loss: 1.1271157264709473\n",
      "Loss: 0.9614619016647339\n",
      "Loss: 1.0309656858444214\n",
      "Loss: 1.0088938474655151\n",
      "Loss: 1.0545806884765625\n",
      "Loss: 1.0814436674118042\n",
      "Loss: 1.0519778728485107\n",
      "Loss: 1.082205891609192\n",
      "Loss: 1.0195375680923462\n",
      "Loss: 1.0621669292449951\n",
      "Loss: 1.1185580492019653\n",
      "Loss: 1.085239291191101\n",
      "Loss: 1.0996938943862915\n",
      "Loss: 1.1054575443267822\n",
      "Loss: 1.0782543420791626\n",
      "Loss: 1.0753532648086548\n",
      "Loss: 1.0803908109664917\n",
      "Loss: 1.0880032777786255\n",
      "Loss: 1.0808053016662598\n",
      "Loss: 1.0358028411865234\n",
      "Loss: 1.0907138586044312\n",
      "Loss: 1.0538291931152344\n",
      "Loss: 1.0348716974258423\n",
      "Loss: 1.1047636270523071\n",
      "Loss: 1.0830038785934448\n",
      "Loss: 1.1054006814956665\n",
      "Loss: 1.1122359037399292\n",
      "Loss: 1.0643041133880615\n",
      "Loss: 1.0137059688568115\n",
      "Loss: 1.126063346862793\n",
      "Loss: 1.083945870399475\n",
      "Loss: 1.0224910974502563\n",
      "Loss: 1.0884969234466553\n",
      "Loss: 1.0554842948913574\n",
      "Loss: 1.064774990081787\n",
      "Loss: 1.1329089403152466\n",
      "Loss: 1.012653112411499\n",
      "Loss: 1.0921186208724976\n",
      "Loss: 1.112541675567627\n",
      "Loss: 1.0975874662399292\n",
      "Loss: 1.075581669807434\n",
      "Loss: 1.0796886682510376\n",
      "Loss: 1.0071513652801514\n",
      "Loss: 1.1158461570739746\n",
      "Loss: 1.1629611253738403\n",
      "Loss: 1.0634911060333252\n",
      "Loss: 0.994990885257721\n",
      "Loss: 1.0536086559295654\n",
      "Loss: 1.011282205581665\n",
      "Loss: 1.146714448928833\n",
      "Loss: 1.0457531213760376\n",
      "Loss: 1.0825670957565308\n",
      "Loss: 1.0696954727172852\n",
      "Loss: 1.035690188407898\n",
      "Loss: 1.092141032218933\n",
      "Loss: 1.1166952848434448\n",
      "Loss: 1.0853267908096313\n",
      "Loss: 1.0872317552566528\n",
      "Loss: 1.0827374458312988\n",
      "Loss: 1.056933879852295\n",
      "Loss: 1.0968964099884033\n",
      "Loss: 1.0433183908462524\n",
      "Loss: 1.1462196111679077\n",
      "Loss: 1.1112009286880493\n",
      "Loss: 1.0579713582992554\n",
      "Loss: 1.016039490699768\n",
      "Loss: 1.0594165325164795\n",
      "Loss: 1.0943875312805176\n",
      "Loss: 1.0623130798339844\n",
      "Loss: 1.0740374326705933\n",
      "Loss: 1.035546898841858\n",
      "Loss: 1.045803189277649\n",
      "Loss: 1.1020703315734863\n",
      "Loss: 1.0467532873153687\n",
      "Loss: 1.1637818813323975\n",
      "Loss: 1.102742075920105\n",
      "Loss: 1.07343590259552\n",
      "Loss: 1.0706621408462524\n",
      "Loss: 1.1099433898925781\n",
      "Loss: 0.9935699105262756\n",
      "Loss: 1.063449740409851\n",
      "Loss: 0.9941732287406921\n",
      "Loss: 1.0553393363952637\n",
      "Loss: 1.1206004619598389\n",
      "Loss: 1.0165290832519531\n",
      "Loss: 1.0815553665161133\n",
      "Loss: 1.08976149559021\n",
      "Loss: 1.0543380975723267\n",
      "Loss: 1.0270743370056152\n",
      "Loss: 1.079909086227417\n",
      "Loss: 1.0629901885986328\n",
      "Loss: 1.0910042524337769\n",
      "Loss: 1.0721875429153442\n",
      "Loss: 1.015425682067871\n",
      "Loss: 1.0111713409423828\n",
      "Loss: 1.1255812644958496\n",
      "Loss: 1.062088131904602\n",
      "Loss: 1.0996899604797363\n",
      "Loss: 1.0507256984710693\n",
      "Loss: 1.0811517238616943\n",
      "Loss: 1.1013189554214478\n",
      "Loss: 1.0769116878509521\n",
      "Loss: 1.091524362564087\n",
      "Loss: 0.9989905953407288\n",
      "Loss: 1.053309440612793\n",
      "Loss: 1.0432136058807373\n",
      "Loss: 1.1464512348175049\n",
      "Loss: 1.1296939849853516\n",
      "Loss: 1.0172542333602905\n",
      "Loss: 1.1168259382247925\n",
      "Loss: 1.1115078926086426\n",
      "Loss: 1.0285136699676514\n",
      "Loss: 1.1259512901306152\n",
      "Loss: 1.046285629272461\n",
      "Loss: 1.1280725002288818\n",
      "Loss: 1.0875705480575562\n",
      "Loss: 1.0412366390228271\n",
      "Loss: 1.0338077545166016\n",
      "Loss: 1.1183191537857056\n",
      "Loss: 1.0247715711593628\n",
      "Loss: 1.0863475799560547\n",
      "Loss: 1.0577723979949951\n",
      "Loss: 1.014971375465393\n",
      "Loss: 1.106630563735962\n",
      "Loss: 1.0084670782089233\n",
      "Loss: 1.0860655307769775\n",
      "Loss: 1.0776156187057495\n",
      "Loss: 1.126336932182312\n",
      "Loss: 1.0677682161331177\n",
      "Loss: 1.1094532012939453\n",
      "Loss: 1.0645610094070435\n",
      "Loss: 1.0654261112213135\n",
      "Loss: 1.0381221771240234\n",
      "Loss: 1.1393064260482788\n",
      "Loss: 1.0697733163833618\n",
      "Loss: 1.0518617630004883\n",
      "Loss: 1.0289499759674072\n",
      "Loss: 1.0619957447052002\n",
      "Loss: 1.0454411506652832\n",
      "Loss: 1.0736664533615112\n",
      "Loss: 1.0179826021194458\n",
      "Loss: 1.0310512781143188\n",
      "Loss: 0.9721316695213318\n",
      "Loss: 1.1031043529510498\n",
      "Loss: 1.0374351739883423\n",
      "Loss: 1.1437346935272217\n",
      "Loss: 1.034774899482727\n",
      "Loss: 1.0266939401626587\n",
      "Loss: 1.052590012550354\n",
      "Loss: 1.0692740678787231\n",
      "Loss: 1.0324541330337524\n",
      "Loss: 1.0432565212249756\n",
      "Loss: 1.0722240209579468\n",
      "Loss: 1.1508175134658813\n",
      "Loss: 1.0430315732955933\n",
      "Loss: 1.0744352340698242\n",
      "Loss: 1.0505766868591309\n",
      "Loss: 1.0959419012069702\n",
      "Loss: 1.0859545469284058\n",
      "Loss: 1.095428466796875\n",
      "Loss: 1.0535235404968262\n",
      "Loss: 1.0569044351577759\n",
      "Loss: 1.1065481901168823\n",
      "Loss: 1.0945268869400024\n",
      "Loss: 1.1258772611618042\n",
      "Loss: 1.137242317199707\n",
      "Loss: 1.0253578424453735\n",
      "Loss: 1.0488650798797607\n",
      "Loss: 1.1179749965667725\n",
      "Loss: 1.0720393657684326\n",
      "Loss: 1.1184310913085938\n",
      "Loss: 1.0806928873062134\n",
      "Loss: 1.0485124588012695\n",
      "Loss: 1.059395432472229\n",
      "Loss: 1.0571564435958862\n",
      "Loss: 1.0423235893249512\n",
      "Loss: 1.0678443908691406\n",
      "Loss: 1.1567553281784058\n",
      "Loss: 1.1118662357330322\n",
      "Loss: 1.1004499197006226\n",
      "Loss: 1.1004594564437866\n",
      "Loss: 1.021828293800354\n",
      "Loss: 1.0991127490997314\n",
      "Loss: 1.1036157608032227\n",
      "Loss: 1.1728730201721191\n",
      "Loss: 1.153320074081421\n",
      "Loss: 1.1008522510528564\n",
      "Loss: 1.0735608339309692\n",
      "Loss: 0.9974030256271362\n",
      "Loss: 1.0843122005462646\n",
      "Loss: 1.0531647205352783\n",
      "Loss: 0.9987208843231201\n",
      "Loss: 1.0550322532653809\n",
      "Loss: 1.0962167978286743\n",
      "Loss: 1.062321424484253\n",
      "Loss: 1.044154405593872\n",
      "Loss: 1.0320658683776855\n",
      "Loss: 1.0599379539489746\n",
      "Loss: 1.0621739625930786\n",
      "Loss: 1.0025842189788818\n",
      "Loss: 1.0443249940872192\n",
      "Loss: 1.0056712627410889\n",
      "Loss: 1.1445960998535156\n",
      "Loss: 1.0434232950210571\n",
      "Loss: 1.125465989112854\n",
      "Loss: 1.0277202129364014\n",
      "Loss: 1.0894395112991333\n",
      "Loss: 1.0619531869888306\n",
      "Loss: 1.0634863376617432\n",
      "Loss: 1.0191043615341187\n",
      "Loss: 1.000895619392395\n",
      "Loss: 1.1196234226226807\n",
      "Loss: 1.0632035732269287\n",
      "Loss: 1.1032021045684814\n",
      "-------- \n",
      " Epoch 1: Average Loss: 1.0710276365280151\n",
      "Epoch 3\n",
      "-----------------------\n",
      "Loss: 1.1516844034194946\n",
      "Loss: 1.091439962387085\n",
      "Loss: 1.0602978467941284\n",
      "Loss: 1.002220630645752\n",
      "Loss: 1.0230463743209839\n",
      "Loss: 1.108976125717163\n",
      "Loss: 1.1305791139602661\n",
      "Loss: 1.0663145780563354\n",
      "Loss: 1.0721677541732788\n",
      "Loss: 1.1569228172302246\n",
      "Loss: 1.1593044996261597\n",
      "Loss: 1.0444799661636353\n",
      "Loss: 1.0717928409576416\n",
      "Loss: 1.1177656650543213\n",
      "Loss: 1.101318597793579\n",
      "Loss: 1.1335712671279907\n",
      "Loss: 1.1133426427841187\n",
      "Loss: 1.072243094444275\n",
      "Loss: 1.0983617305755615\n",
      "Loss: 1.0015361309051514\n",
      "Loss: 1.0021485090255737\n",
      "Loss: 1.1282885074615479\n",
      "Loss: 0.9926636219024658\n",
      "Loss: 1.1002134084701538\n",
      "Loss: 1.032729148864746\n",
      "Loss: 1.0741082429885864\n",
      "Loss: 1.076009750366211\n",
      "Loss: 1.0364582538604736\n",
      "Loss: 1.0482522249221802\n",
      "Loss: 1.0858514308929443\n",
      "Loss: 1.0062810182571411\n",
      "Loss: 1.0885564088821411\n",
      "Loss: 1.0749937295913696\n",
      "Loss: 1.053013563156128\n",
      "Loss: 1.1336451768875122\n",
      "Loss: 1.0370298624038696\n",
      "Loss: 1.1093270778656006\n",
      "Loss: 1.116721510887146\n",
      "Loss: 1.0358798503875732\n",
      "Loss: 1.0755759477615356\n",
      "Loss: 1.0997154712677002\n",
      "Loss: 1.028725028038025\n",
      "Loss: 1.0540286302566528\n",
      "Loss: 1.0813733339309692\n",
      "Loss: 1.059095025062561\n",
      "Loss: 1.0643608570098877\n",
      "Loss: 1.1081492900848389\n",
      "Loss: 1.0917543172836304\n",
      "Loss: 1.0780655145645142\n",
      "Loss: 0.9989702105522156\n",
      "Loss: 1.0841457843780518\n",
      "Loss: 1.0039355754852295\n",
      "Loss: 0.9866218566894531\n",
      "Loss: 1.0814967155456543\n",
      "Loss: 1.09408700466156\n",
      "Loss: 1.0044441223144531\n",
      "Loss: 1.0797502994537354\n",
      "Loss: 1.0277843475341797\n",
      "Loss: 1.0308805704116821\n",
      "Loss: 1.033124327659607\n",
      "Loss: 1.0543357133865356\n",
      "Loss: 1.1176791191101074\n",
      "Loss: 1.0944108963012695\n",
      "Loss: 1.051102876663208\n",
      "Loss: 1.1005414724349976\n",
      "Loss: 1.0368967056274414\n",
      "Loss: 1.0574580430984497\n",
      "Loss: 1.032631754875183\n",
      "Loss: 1.1542497873306274\n",
      "Loss: 1.1022038459777832\n",
      "Loss: 1.1041290760040283\n",
      "Loss: 1.0801610946655273\n",
      "Loss: 1.05222749710083\n",
      "Loss: 1.0036137104034424\n",
      "Loss: 1.0567442178726196\n",
      "Loss: 1.0722841024398804\n",
      "Loss: 1.015025019645691\n",
      "Loss: 1.1361944675445557\n",
      "Loss: 1.0968132019042969\n",
      "Loss: 1.0888557434082031\n",
      "Loss: 1.0544224977493286\n",
      "Loss: 1.0381650924682617\n",
      "Loss: 1.0613652467727661\n",
      "Loss: 1.1391018629074097\n",
      "Loss: 1.0629700422286987\n",
      "Loss: 1.11863374710083\n",
      "Loss: 1.0457515716552734\n",
      "Loss: 1.068463683128357\n",
      "Loss: 1.0907540321350098\n",
      "Loss: 1.0441069602966309\n",
      "Loss: 1.0891424417495728\n",
      "Loss: 1.084653377532959\n",
      "Loss: 1.0956271886825562\n",
      "Loss: 1.127691388130188\n",
      "Loss: 1.0293786525726318\n",
      "Loss: 1.043430209159851\n",
      "Loss: 1.1090863943099976\n",
      "Loss: 1.0791685581207275\n",
      "Loss: 1.0352119207382202\n",
      "Loss: 1.0722459554672241\n",
      "Loss: 1.1194133758544922\n",
      "Loss: 1.1256321668624878\n",
      "Loss: 1.0554686784744263\n",
      "Loss: 1.1527636051177979\n",
      "Loss: 1.023236632347107\n",
      "Loss: 1.1104974746704102\n",
      "Loss: 1.0663689374923706\n",
      "Loss: 1.0747878551483154\n",
      "Loss: 1.090958833694458\n",
      "Loss: 1.0581645965576172\n",
      "Loss: 1.0535849332809448\n",
      "Loss: 1.0918982028961182\n",
      "Loss: 1.0244693756103516\n",
      "Loss: 1.081798791885376\n",
      "Loss: 1.07846999168396\n",
      "Loss: 1.0458183288574219\n",
      "Loss: 1.170955777168274\n",
      "Loss: 1.0711325407028198\n",
      "Loss: 1.091132402420044\n",
      "Loss: 1.1083799600601196\n",
      "Loss: 1.0975196361541748\n",
      "Loss: 1.0811532735824585\n",
      "Loss: 1.0346343517303467\n",
      "Loss: 1.0987639427185059\n",
      "Loss: 1.0532680749893188\n",
      "Loss: 1.044763445854187\n",
      "Loss: 1.0709459781646729\n",
      "Loss: 1.0062237977981567\n",
      "Loss: 1.1185877323150635\n",
      "Loss: 1.0514942407608032\n",
      "Loss: 1.080929160118103\n",
      "Loss: 1.088249683380127\n",
      "Loss: 1.1188859939575195\n",
      "Loss: 1.0630158185958862\n",
      "Loss: 1.0517723560333252\n",
      "Loss: 1.0031406879425049\n",
      "Loss: 1.0631024837493896\n",
      "Loss: 1.0593962669372559\n",
      "Loss: 1.0112264156341553\n",
      "Loss: 1.115925908088684\n",
      "Loss: 1.099806785583496\n",
      "Loss: 1.0974825620651245\n",
      "Loss: 1.063732385635376\n",
      "Loss: 1.0806390047073364\n",
      "Loss: 1.0837976932525635\n",
      "Loss: 1.1000645160675049\n",
      "Loss: 1.1855640411376953\n",
      "Loss: 1.0533536672592163\n",
      "Loss: 1.0547147989273071\n",
      "Loss: 1.0441433191299438\n",
      "Loss: 1.1422985792160034\n",
      "Loss: 1.1246910095214844\n",
      "Loss: 1.0708653926849365\n",
      "Loss: 1.1530989408493042\n",
      "Loss: 1.0356563329696655\n",
      "Loss: 1.0632760524749756\n",
      "Loss: 1.1155139207839966\n",
      "Loss: 1.0531316995620728\n",
      "Loss: 1.1305121183395386\n",
      "Loss: 1.0447964668273926\n",
      "Loss: 1.085239291191101\n",
      "Loss: 1.0994007587432861\n",
      "Loss: 1.098336935043335\n",
      "Loss: 1.1346664428710938\n",
      "Loss: 1.0514980554580688\n",
      "Loss: 1.1524600982666016\n",
      "Loss: 1.0792648792266846\n",
      "Loss: 1.1133792400360107\n",
      "Loss: 1.0118640661239624\n",
      "Loss: 0.9854503273963928\n",
      "Loss: 1.0267245769500732\n",
      "Loss: 1.0299164056777954\n",
      "Loss: 1.0675435066223145\n",
      "Loss: 0.9667776226997375\n",
      "Loss: 1.0451140403747559\n",
      "Loss: 1.0071954727172852\n",
      "Loss: 1.0815277099609375\n",
      "Loss: 1.0361270904541016\n",
      "Loss: 1.0968176126480103\n",
      "Loss: 1.1176198720932007\n",
      "Loss: 1.1479969024658203\n",
      "Loss: 1.0637673139572144\n",
      "Loss: 1.1227689981460571\n",
      "Loss: 1.0452516078948975\n",
      "Loss: 1.0985727310180664\n",
      "Loss: 1.064200520515442\n",
      "Loss: 0.9989979267120361\n",
      "Loss: 1.106987476348877\n",
      "Loss: 0.9911754131317139\n",
      "Loss: 0.9946749210357666\n",
      "Loss: 1.156345009803772\n",
      "Loss: 1.0472111701965332\n",
      "Loss: 1.1007939577102661\n",
      "Loss: 1.038948893547058\n",
      "Loss: 1.0858218669891357\n",
      "Loss: 1.0577330589294434\n",
      "Loss: 1.102607250213623\n",
      "Loss: 1.0902504920959473\n",
      "Loss: 1.1374950408935547\n",
      "Loss: 1.0400663614273071\n",
      "Loss: 1.0373516082763672\n",
      "Loss: 1.0815447568893433\n",
      "Loss: 1.0084130764007568\n",
      "Loss: 1.0082364082336426\n",
      "Loss: 1.0903617143630981\n",
      "Loss: 1.0364227294921875\n",
      "Loss: 1.0548608303070068\n",
      "Loss: 1.1059902906417847\n",
      "Loss: 1.1879265308380127\n",
      "Loss: 1.043251872062683\n",
      "Loss: 0.9670534133911133\n",
      "Loss: 1.120675802230835\n",
      "Loss: 1.0225849151611328\n",
      "Loss: 1.0913887023925781\n",
      "Loss: 1.055967926979065\n",
      "Loss: 1.0810892581939697\n",
      "Loss: 1.074851632118225\n",
      "Loss: 1.0739102363586426\n",
      "Loss: 1.081599235534668\n",
      "Loss: 1.0540891885757446\n",
      "Loss: 1.0281813144683838\n",
      "Loss: 1.0939934253692627\n",
      "Loss: 1.1640419960021973\n",
      "Loss: 1.0678765773773193\n",
      "Loss: 1.0987374782562256\n",
      "Loss: 1.0778918266296387\n",
      "Loss: 1.119698405265808\n",
      "Loss: 1.0804232358932495\n",
      "Loss: 1.034358263015747\n",
      "Loss: 1.0746874809265137\n",
      "Loss: 1.0698661804199219\n",
      "Loss: 1.0974067449569702\n",
      "Loss: 1.0457072257995605\n",
      "Loss: 1.127139687538147\n",
      "Loss: 1.1067180633544922\n",
      "Loss: 1.022096037864685\n",
      "Loss: 1.0526326894760132\n",
      "Loss: 1.0646806955337524\n",
      "Loss: 1.150961995124817\n",
      "Loss: 1.048643708229065\n",
      "Loss: 1.0536388158798218\n",
      "Loss: 1.08992600440979\n",
      "Loss: 1.0805583000183105\n",
      "Loss: 1.033586859703064\n",
      "Loss: 0.9978117346763611\n",
      "Loss: 1.1074168682098389\n",
      "-------- \n",
      " Epoch 2: Average Loss: 1.0708410739898682\n",
      "Epoch 4\n",
      "-----------------------\n",
      "Loss: 1.0448532104492188\n",
      "Loss: 1.0758535861968994\n",
      "Loss: 1.0717312097549438\n",
      "Loss: 1.0466502904891968\n",
      "Loss: 1.0970406532287598\n",
      "Loss: 1.0890374183654785\n",
      "Loss: 0.9728923439979553\n",
      "Loss: 1.1111139059066772\n",
      "Loss: 1.0353986024856567\n",
      "Loss: 1.086931824684143\n",
      "Loss: 1.062818169593811\n",
      "Loss: 1.0823845863342285\n",
      "Loss: 1.0420141220092773\n",
      "Loss: 1.1102306842803955\n",
      "Loss: 1.0998834371566772\n",
      "Loss: 1.1378390789031982\n",
      "Loss: 1.0896495580673218\n",
      "Loss: 1.0639568567276\n",
      "Loss: 1.0175002813339233\n",
      "Loss: 1.1206588745117188\n",
      "Loss: 1.0909756422042847\n",
      "Loss: 1.0450695753097534\n",
      "Loss: 1.1307264566421509\n",
      "Loss: 1.16056489944458\n",
      "Loss: 1.130436897277832\n",
      "Loss: 1.0424245595932007\n",
      "Loss: 1.1094416379928589\n",
      "Loss: 1.0198094844818115\n",
      "Loss: 1.0357547998428345\n",
      "Loss: 1.0601094961166382\n",
      "Loss: 1.0570036172866821\n",
      "Loss: 1.039170265197754\n",
      "Loss: 1.089564323425293\n",
      "Loss: 1.0749986171722412\n",
      "Loss: 1.0913407802581787\n",
      "Loss: 1.099359154701233\n",
      "Loss: 1.0376183986663818\n",
      "Loss: 1.0549291372299194\n",
      "Loss: 1.174012541770935\n",
      "Loss: 1.0999407768249512\n",
      "Loss: 1.1510435342788696\n",
      "Loss: 1.129164695739746\n",
      "Loss: 1.0885720252990723\n",
      "Loss: 1.0809121131896973\n",
      "Loss: 1.0151214599609375\n",
      "Loss: 1.063996434211731\n",
      "Loss: 1.0290632247924805\n",
      "Loss: 1.105588436126709\n",
      "Loss: 0.9896097779273987\n",
      "Loss: 1.0256268978118896\n",
      "Loss: 1.0445520877838135\n",
      "Loss: 1.1450393199920654\n",
      "Loss: 1.0462743043899536\n",
      "Loss: 1.0954617261886597\n",
      "Loss: 0.959735631942749\n",
      "Loss: 1.053743839263916\n",
      "Loss: 1.0894509553909302\n",
      "Loss: 1.0810654163360596\n",
      "Loss: 1.0710116624832153\n",
      "Loss: 1.0998021364212036\n",
      "Loss: 1.040204644203186\n",
      "Loss: 1.0273363590240479\n",
      "Loss: 1.0912022590637207\n",
      "Loss: 1.0821770429611206\n",
      "Loss: 1.0039448738098145\n",
      "Loss: 1.0685656070709229\n",
      "Loss: 1.1278167963027954\n",
      "Loss: 1.127983570098877\n",
      "Loss: 1.060673475265503\n",
      "Loss: 1.099874496459961\n",
      "Loss: 1.0701984167099\n",
      "Loss: 1.0885673761367798\n",
      "Loss: 1.0355737209320068\n",
      "Loss: 1.0976485013961792\n",
      "Loss: 1.0457130670547485\n",
      "Loss: 1.0696204900741577\n",
      "Loss: 1.0514483451843262\n",
      "Loss: 1.0992615222930908\n",
      "Loss: 1.1095691919326782\n",
      "Loss: 1.0082365274429321\n",
      "Loss: 1.1076107025146484\n",
      "Loss: 1.0324643850326538\n",
      "Loss: 1.18813157081604\n",
      "Loss: 1.1517924070358276\n",
      "Loss: 1.0057854652404785\n",
      "Loss: 1.0741008520126343\n",
      "Loss: 1.0801124572753906\n",
      "Loss: 1.0485955476760864\n",
      "Loss: 1.0531092882156372\n",
      "Loss: 1.1008882522583008\n",
      "Loss: 1.0570430755615234\n",
      "Loss: 1.0757975578308105\n",
      "Loss: 1.108813762664795\n",
      "Loss: 1.1210367679595947\n",
      "Loss: 1.0457251071929932\n",
      "Loss: 1.0357649326324463\n",
      "Loss: 1.113896369934082\n",
      "Loss: 1.0204520225524902\n",
      "Loss: 1.052616834640503\n",
      "Loss: 1.0223480463027954\n",
      "Loss: 1.1266086101531982\n",
      "Loss: 1.0387502908706665\n",
      "Loss: 1.0503751039505005\n",
      "Loss: 1.1402487754821777\n",
      "Loss: 1.0636744499206543\n",
      "Loss: 1.0726306438446045\n",
      "Loss: 1.1014130115509033\n",
      "Loss: 1.0895140171051025\n",
      "Loss: 1.1012626886367798\n",
      "Loss: 1.1019947528839111\n",
      "Loss: 1.0652202367782593\n",
      "Loss: 1.0966227054595947\n",
      "Loss: 1.0535424947738647\n",
      "Loss: 1.0962532758712769\n",
      "Loss: 1.0715547800064087\n",
      "Loss: 1.064738392829895\n",
      "Loss: 1.0620908737182617\n",
      "Loss: 1.0645626783370972\n",
      "Loss: 0.9953934550285339\n",
      "Loss: 1.019090175628662\n",
      "Loss: 1.071672797203064\n",
      "Loss: 1.1082803010940552\n",
      "Loss: 1.0905616283416748\n",
      "Loss: 1.0622700452804565\n",
      "Loss: 1.0966209173202515\n",
      "Loss: 1.0228193998336792\n",
      "Loss: 1.0897611379623413\n",
      "Loss: 1.0067037343978882\n",
      "Loss: 1.0727343559265137\n",
      "Loss: 1.0651503801345825\n",
      "Loss: 1.0506932735443115\n",
      "Loss: 1.1209274530410767\n",
      "Loss: 1.0504264831542969\n",
      "Loss: 1.0325233936309814\n",
      "Loss: 1.0891045331954956\n",
      "Loss: 1.0474931001663208\n",
      "Loss: 1.0722731351852417\n",
      "Loss: 1.1008867025375366\n",
      "Loss: 1.0802347660064697\n",
      "Loss: 1.0316541194915771\n",
      "Loss: 1.0649645328521729\n",
      "Loss: 1.0027146339416504\n",
      "Loss: 1.0752480030059814\n",
      "Loss: 1.120138168334961\n",
      "Loss: 1.0623672008514404\n",
      "Loss: 1.0534214973449707\n",
      "Loss: 1.1671546697616577\n",
      "Loss: 1.0997413396835327\n",
      "Loss: 1.0806387662887573\n",
      "Loss: 1.0336802005767822\n",
      "Loss: 1.1498843431472778\n",
      "Loss: 1.135321855545044\n",
      "Loss: 1.074945092201233\n",
      "Loss: 1.086485505104065\n",
      "Loss: 1.0559519529342651\n",
      "Loss: 0.9910378456115723\n",
      "Loss: 1.028704285621643\n",
      "Loss: 1.0835082530975342\n",
      "Loss: 1.1444118022918701\n",
      "Loss: 1.0733654499053955\n",
      "Loss: 1.0206794738769531\n",
      "Loss: 1.0578538179397583\n",
      "Loss: 1.0457794666290283\n",
      "Loss: 1.072779655456543\n",
      "Loss: 1.1169428825378418\n",
      "Loss: 0.9945709705352783\n",
      "Loss: 1.0576680898666382\n",
      "Loss: 1.0996900796890259\n",
      "Loss: 1.1137232780456543\n",
      "Loss: 1.0814975500106812\n",
      "Loss: 1.0177223682403564\n",
      "Loss: 1.0623606443405151\n",
      "Loss: 1.0441523790359497\n",
      "Loss: 1.0753690004348755\n",
      "Loss: 1.0950345993041992\n",
      "Loss: 1.0422680377960205\n",
      "Loss: 1.0641783475875854\n",
      "Loss: 0.9714279770851135\n",
      "Loss: 1.0812451839447021\n",
      "Loss: 1.0628490447998047\n",
      "Loss: 1.1065425872802734\n",
      "Loss: 1.0364274978637695\n",
      "Loss: 1.111104130744934\n",
      "Loss: 1.0166183710098267\n",
      "Loss: 1.0898706912994385\n",
      "Loss: 1.0580264329910278\n",
      "Loss: 1.1193294525146484\n",
      "Loss: 1.0698390007019043\n",
      "Loss: 1.0164265632629395\n",
      "Loss: 1.1503390073776245\n",
      "Loss: 1.136874794960022\n",
      "Loss: 1.0158665180206299\n",
      "Loss: 1.0839723348617554\n",
      "Loss: 1.0331283807754517\n",
      "Loss: 1.072340488433838\n",
      "Loss: 1.0538759231567383\n",
      "Loss: 1.0937470197677612\n",
      "Loss: 1.090065836906433\n",
      "Loss: 1.0989311933517456\n",
      "Loss: 1.0853716135025024\n",
      "Loss: 1.1286461353302002\n",
      "Loss: 1.0325579643249512\n",
      "Loss: 1.1173640489578247\n",
      "Loss: 1.1190072298049927\n",
      "Loss: 1.1356436014175415\n",
      "Loss: 1.0698915719985962\n",
      "Loss: 1.0508493185043335\n",
      "Loss: 1.0647003650665283\n",
      "Loss: 1.1294567584991455\n",
      "Loss: 1.0971529483795166\n",
      "Loss: 1.0875533819198608\n",
      "Loss: 1.0833930969238281\n",
      "Loss: 1.0938905477523804\n",
      "Loss: 1.0210496187210083\n",
      "Loss: 1.0787705183029175\n",
      "Loss: 1.1066298484802246\n",
      "Loss: 1.072079062461853\n",
      "Loss: 1.0465127229690552\n",
      "Loss: 1.015457272529602\n",
      "Loss: 1.0355148315429688\n",
      "Loss: 1.11483633518219\n",
      "Loss: 1.122739315032959\n",
      "Loss: 1.0895025730133057\n",
      "Loss: 1.062867283821106\n",
      "Loss: 1.0173226594924927\n",
      "Loss: 1.143843650817871\n",
      "Loss: 1.079914927482605\n",
      "Loss: 1.0233612060546875\n",
      "Loss: 1.0144329071044922\n",
      "Loss: 1.0963085889816284\n",
      "Loss: 1.0795247554779053\n",
      "Loss: 1.0977795124053955\n",
      "Loss: 1.0441758632659912\n",
      "Loss: 1.0296354293823242\n",
      "Loss: 1.0904918909072876\n",
      "Loss: 1.0918068885803223\n",
      "Loss: 1.019333839416504\n",
      "Loss: 1.0757131576538086\n",
      "Loss: 1.0757955312728882\n",
      "Loss: 1.1437067985534668\n",
      "Loss: 1.0781100988388062\n",
      "Loss: 1.0163531303405762\n",
      "Loss: 1.0925036668777466\n",
      "Loss: 1.0609585046768188\n",
      "Loss: 1.0736000537872314\n",
      "Loss: 1.0581042766571045\n",
      "-------- \n",
      " Epoch 3: Average Loss: 1.0709152221679688\n",
      "Epoch 5\n",
      "-----------------------\n",
      "Loss: 1.135656714439392\n",
      "Loss: 1.0535247325897217\n",
      "Loss: 1.0641913414001465\n",
      "Loss: 1.1276518106460571\n",
      "Loss: 1.033011555671692\n",
      "Loss: 1.028643250465393\n",
      "Loss: 1.0762273073196411\n",
      "Loss: 1.1154621839523315\n",
      "Loss: 1.0698270797729492\n",
      "Loss: 1.0331379175186157\n",
      "Loss: 1.1251165866851807\n",
      "Loss: 1.1951771974563599\n",
      "Loss: 1.0358506441116333\n",
      "Loss: 1.045811653137207\n",
      "Loss: 1.0509604215621948\n",
      "Loss: 1.1153429746627808\n",
      "Loss: 1.0819661617279053\n",
      "Loss: 1.0625970363616943\n",
      "Loss: 1.0576069355010986\n",
      "Loss: 1.0677262544631958\n",
      "Loss: 1.0729328393936157\n",
      "Loss: 1.0241724252700806\n",
      "Loss: 1.0247496366500854\n",
      "Loss: 1.139583945274353\n",
      "Loss: 1.0704004764556885\n",
      "Loss: 1.043471097946167\n",
      "Loss: 1.0331393480300903\n",
      "Loss: 1.026796579360962\n",
      "Loss: 1.047179102897644\n",
      "Loss: 1.020073413848877\n",
      "Loss: 0.9871718883514404\n",
      "Loss: 1.031864881515503\n",
      "Loss: 1.111333966255188\n",
      "Loss: 1.0821939706802368\n",
      "Loss: 1.079330325126648\n",
      "Loss: 0.9977328777313232\n",
      "Loss: 1.0238029956817627\n",
      "Loss: 1.0526829957962036\n",
      "Loss: 1.0399357080459595\n",
      "Loss: 1.0686054229736328\n",
      "Loss: 1.0247963666915894\n",
      "Loss: 1.063906192779541\n",
      "Loss: 1.0704519748687744\n",
      "Loss: 1.0547410249710083\n",
      "Loss: 1.1256625652313232\n",
      "Loss: 1.1170971393585205\n",
      "Loss: 1.0839567184448242\n",
      "Loss: 1.0726125240325928\n",
      "Loss: 1.0058863162994385\n",
      "Loss: 1.1056101322174072\n",
      "Loss: 0.9683248400688171\n",
      "Loss: 1.0969206094741821\n",
      "Loss: 1.099354863166809\n",
      "Loss: 1.0128308534622192\n",
      "Loss: 1.0446655750274658\n",
      "Loss: 1.0762180089950562\n",
      "Loss: 1.0346213579177856\n",
      "Loss: 1.0892236232757568\n",
      "Loss: 1.1140507459640503\n",
      "Loss: 1.137097716331482\n",
      "Loss: 1.103078007698059\n",
      "Loss: 1.0825650691986084\n",
      "Loss: 1.1488970518112183\n",
      "Loss: 1.071117639541626\n",
      "Loss: 1.1220910549163818\n",
      "Loss: 1.0871360301971436\n",
      "Loss: 1.0790913105010986\n",
      "Loss: 1.0785307884216309\n",
      "Loss: 1.1256128549575806\n",
      "Loss: 1.0450439453125\n",
      "Loss: 1.0892794132232666\n",
      "Loss: 1.0078669786453247\n",
      "Loss: 1.0570365190505981\n",
      "Loss: 1.0985640287399292\n",
      "Loss: 0.9581553339958191\n",
      "Loss: 1.0397648811340332\n",
      "Loss: 1.0618139505386353\n",
      "Loss: 0.991926908493042\n",
      "Loss: 1.0942883491516113\n",
      "Loss: 1.0745011568069458\n",
      "Loss: 1.1419670581817627\n",
      "Loss: 1.0819976329803467\n",
      "Loss: 1.05652916431427\n",
      "Loss: 0.9963380694389343\n",
      "Loss: 1.0620123147964478\n",
      "Loss: 1.0418332815170288\n",
      "Loss: 1.083592176437378\n",
      "Loss: 1.0727087259292603\n",
      "Loss: 1.0462547540664673\n",
      "Loss: 1.0386617183685303\n",
      "Loss: 1.098420262336731\n",
      "Loss: 1.0472923517227173\n",
      "Loss: 0.9768548607826233\n",
      "Loss: 1.0331761837005615\n",
      "Loss: 1.037150263786316\n",
      "Loss: 1.0878465175628662\n",
      "Loss: 1.0160999298095703\n",
      "Loss: 1.0926344394683838\n",
      "Loss: 1.1158905029296875\n",
      "Loss: 1.0898257493972778\n",
      "Loss: 1.071921706199646\n",
      "Loss: 1.083188533782959\n",
      "Loss: 1.0724544525146484\n",
      "Loss: 1.052341341972351\n",
      "Loss: 1.0546927452087402\n",
      "Loss: 1.111362338066101\n",
      "Loss: 0.9828172326087952\n",
      "Loss: 1.1264418363571167\n",
      "Loss: 1.1209385395050049\n",
      "Loss: 1.1102991104125977\n",
      "Loss: 1.1163626909255981\n",
      "Loss: 1.119760513305664\n",
      "Loss: 1.0301145315170288\n",
      "Loss: 1.0618114471435547\n",
      "Loss: 1.044527530670166\n",
      "Loss: 1.091294288635254\n",
      "Loss: 1.164471983909607\n",
      "Loss: 1.016021728515625\n",
      "Loss: 1.191780924797058\n",
      "Loss: 1.0736641883850098\n",
      "Loss: 1.0413283109664917\n",
      "Loss: 1.087889552116394\n",
      "Loss: 0.9710684418678284\n",
      "Loss: 1.036073088645935\n",
      "Loss: 1.072873592376709\n",
      "Loss: 1.0089973211288452\n",
      "Loss: 1.063016414642334\n",
      "Loss: 1.1417452096939087\n",
      "Loss: 1.1032525300979614\n",
      "Loss: 1.0886116027832031\n",
      "Loss: 1.1096718311309814\n",
      "Loss: 1.0550572872161865\n",
      "Loss: 1.0840821266174316\n",
      "Loss: 0.990446925163269\n",
      "Loss: 1.0613881349563599\n",
      "Loss: 1.0831547975540161\n",
      "Loss: 1.0471962690353394\n",
      "Loss: 1.092698097229004\n",
      "Loss: 1.0443600416183472\n",
      "Loss: 1.1249234676361084\n",
      "Loss: 1.0696226358413696\n",
      "Loss: 1.0514137744903564\n",
      "Loss: 1.1061832904815674\n",
      "Loss: 1.0386314392089844\n",
      "Loss: 1.0503990650177002\n",
      "Loss: 1.0770752429962158\n",
      "Loss: 1.0224162340164185\n",
      "Loss: 1.0759456157684326\n",
      "Loss: 1.0997074842453003\n",
      "Loss: 1.0593609809875488\n",
      "Loss: 1.1310876607894897\n",
      "Loss: 1.015769124031067\n",
      "Loss: 1.0419666767120361\n",
      "Loss: 1.054231882095337\n",
      "Loss: 1.102902889251709\n",
      "Loss: 1.1038702726364136\n",
      "Loss: 1.083105206489563\n",
      "Loss: 1.0665547847747803\n",
      "Loss: 1.0913885831832886\n",
      "Loss: 1.0316853523254395\n",
      "Loss: 1.0806105136871338\n",
      "Loss: 0.9946578145027161\n",
      "Loss: 1.1357687711715698\n",
      "Loss: 1.0815401077270508\n",
      "Loss: 1.074749231338501\n",
      "Loss: 1.0876470804214478\n",
      "Loss: 1.0919283628463745\n",
      "Loss: 1.0374516248703003\n",
      "Loss: 1.0798614025115967\n",
      "Loss: 1.1020164489746094\n",
      "Loss: 1.0777250528335571\n",
      "Loss: 1.0179734230041504\n",
      "Loss: 1.1045600175857544\n",
      "Loss: 1.0888738632202148\n",
      "Loss: 1.1445765495300293\n",
      "Loss: 1.04923677444458\n",
      "Loss: 1.0295275449752808\n",
      "Loss: 1.0940219163894653\n",
      "Loss: 1.0340299606323242\n",
      "Loss: 1.043580412864685\n",
      "Loss: 1.0248472690582275\n",
      "Loss: 1.0807194709777832\n",
      "Loss: 1.0029542446136475\n",
      "Loss: 1.184728980064392\n",
      "Loss: 1.0240511894226074\n",
      "Loss: 1.0420349836349487\n",
      "Loss: 1.1149600744247437\n",
      "Loss: 1.0285166501998901\n",
      "Loss: 1.044584035873413\n",
      "Loss: 1.1271321773529053\n",
      "Loss: 1.0241475105285645\n",
      "Loss: 1.0448654890060425\n",
      "Loss: 1.0450429916381836\n",
      "Loss: 1.0648725032806396\n",
      "Loss: 1.1195273399353027\n",
      "Loss: 1.0420714616775513\n",
      "Loss: 1.0651180744171143\n",
      "Loss: 1.0520085096359253\n",
      "Loss: 1.0921828746795654\n",
      "Loss: 1.0723470449447632\n",
      "Loss: 1.011278748512268\n",
      "Loss: 1.0257078409194946\n",
      "Loss: 1.0595898628234863\n",
      "Loss: 1.0633436441421509\n",
      "Loss: 1.0631778240203857\n",
      "Loss: 1.0628658533096313\n",
      "Loss: 0.9900100231170654\n",
      "Loss: 1.0992592573165894\n",
      "Loss: 1.0945565700531006\n",
      "Loss: 1.1052041053771973\n",
      "Loss: 1.0616377592086792\n",
      "Loss: 1.1219598054885864\n",
      "Loss: 1.010016918182373\n",
      "Loss: 1.0732450485229492\n",
      "Loss: 1.062207579612732\n",
      "Loss: 1.0781983137130737\n",
      "Loss: 1.0039536952972412\n",
      "Loss: 1.1088414192199707\n",
      "Loss: 1.1790499687194824\n",
      "Loss: 1.031352162361145\n",
      "Loss: 1.0743920803070068\n",
      "Loss: 1.079242467880249\n",
      "Loss: 1.0650302171707153\n",
      "Loss: 1.0366483926773071\n",
      "Loss: 1.0418689250946045\n",
      "Loss: 1.0456852912902832\n",
      "Loss: 1.0796483755111694\n",
      "Loss: 1.0188331604003906\n",
      "Loss: 1.0654534101486206\n",
      "Loss: 1.0736690759658813\n",
      "Loss: 1.1005915403366089\n",
      "Loss: 1.073248267173767\n",
      "Loss: 1.1209992170333862\n",
      "Loss: 1.0109914541244507\n",
      "Loss: 1.0805747509002686\n",
      "Loss: 1.1003477573394775\n",
      "Loss: 1.1361958980560303\n",
      "Loss: 1.0178914070129395\n",
      "Loss: 1.0548629760742188\n",
      "Loss: 1.1188673973083496\n",
      "Loss: 1.037040114402771\n",
      "Loss: 1.0997591018676758\n",
      "Loss: 1.0220508575439453\n",
      "Loss: 1.034407138824463\n",
      "Loss: 1.0780701637268066\n",
      "Loss: 1.0375531911849976\n",
      "-------- \n",
      " Epoch 4: Average Loss: 1.0709139108657837\n",
      "Epoch 6\n",
      "-----------------------\n",
      "Loss: 1.0379955768585205\n",
      "Loss: 1.135972023010254\n",
      "Loss: 1.0429086685180664\n",
      "Loss: 1.0073028802871704\n",
      "Loss: 1.0575145483016968\n",
      "Loss: 1.025464415550232\n",
      "Loss: 0.9958676099777222\n",
      "Loss: 1.0560778379440308\n",
      "Loss: 1.0967161655426025\n",
      "Loss: 1.0840576887130737\n",
      "Loss: 1.1397440433502197\n",
      "Loss: 1.0901436805725098\n",
      "Loss: 1.1268024444580078\n",
      "Loss: 1.0801692008972168\n",
      "Loss: 1.0315216779708862\n",
      "Loss: 1.0622683763504028\n",
      "Loss: 1.0015027523040771\n",
      "Loss: 1.0577988624572754\n",
      "Loss: 1.065484642982483\n",
      "Loss: 0.9962702989578247\n",
      "Loss: 1.0385444164276123\n",
      "Loss: 1.0874807834625244\n",
      "Loss: 1.0850359201431274\n",
      "Loss: 1.1750695705413818\n",
      "Loss: 1.0621780157089233\n",
      "Loss: 1.0689339637756348\n",
      "Loss: 1.124480128288269\n",
      "Loss: 1.037173867225647\n",
      "Loss: 1.0629616975784302\n",
      "Loss: 1.0821367502212524\n",
      "Loss: 1.1217302083969116\n",
      "Loss: 1.068009614944458\n",
      "Loss: 1.1358798742294312\n",
      "Loss: 1.0220468044281006\n",
      "Loss: 1.0618503093719482\n",
      "Loss: 1.1094591617584229\n",
      "Loss: 1.081885814666748\n",
      "Loss: 1.0250025987625122\n",
      "Loss: 1.0531976222991943\n",
      "Loss: 1.0915218591690063\n",
      "Loss: 1.1401047706604004\n",
      "Loss: 1.0796277523040771\n",
      "Loss: 1.0773297548294067\n",
      "Loss: 1.0621061325073242\n",
      "Loss: 1.0542993545532227\n",
      "Loss: 1.0462095737457275\n",
      "Loss: 1.151565432548523\n",
      "Loss: 1.043512225151062\n",
      "Loss: 1.0216916799545288\n",
      "Loss: 1.088128924369812\n",
      "Loss: 1.1476824283599854\n",
      "Loss: 1.0879310369491577\n",
      "Loss: 1.0606656074523926\n",
      "Loss: 1.107079029083252\n",
      "Loss: 1.1095181703567505\n",
      "Loss: 1.1205826997756958\n",
      "Loss: 1.0837122201919556\n",
      "Loss: 1.063652515411377\n",
      "Loss: 1.0821361541748047\n",
      "Loss: 0.9985204339027405\n",
      "Loss: 1.078389048576355\n",
      "Loss: 1.107102632522583\n",
      "Loss: 1.0715675354003906\n",
      "Loss: 1.091190218925476\n",
      "Loss: 1.0606285333633423\n",
      "Loss: 1.120148777961731\n",
      "Loss: 1.0742751359939575\n",
      "Loss: 1.0635310411453247\n",
      "Loss: 1.049839973449707\n",
      "Loss: 1.0324254035949707\n",
      "Loss: 1.0526423454284668\n",
      "Loss: 1.0707578659057617\n",
      "Loss: 1.0611873865127563\n",
      "Loss: 1.0736379623413086\n",
      "Loss: 1.1095123291015625\n",
      "Loss: 0.9778497815132141\n",
      "Loss: 1.1147178411483765\n",
      "Loss: 1.083495020866394\n",
      "Loss: 1.1186155080795288\n",
      "Loss: 1.0600224733352661\n",
      "Loss: 1.0779860019683838\n",
      "Loss: 1.0819977521896362\n",
      "Loss: 1.0731340646743774\n",
      "Loss: 1.0317773818969727\n",
      "Loss: 1.0143295526504517\n",
      "Loss: 1.0718601942062378\n",
      "Loss: 1.0540372133255005\n",
      "Loss: 1.0630018711090088\n",
      "Loss: 1.070534348487854\n",
      "Loss: 1.0843753814697266\n",
      "Loss: 1.0114253759384155\n",
      "Loss: 1.0202267169952393\n",
      "Loss: 1.0798527002334595\n",
      "Loss: 1.1076687574386597\n",
      "Loss: 1.0404301881790161\n",
      "Loss: 1.0810660123825073\n",
      "Loss: 1.0813186168670654\n",
      "Loss: 1.0191532373428345\n",
      "Loss: 1.1373605728149414\n",
      "Loss: 1.0849690437316895\n",
      "Loss: 1.1143825054168701\n",
      "Loss: 1.072435975074768\n",
      "Loss: 1.0716735124588013\n",
      "Loss: 1.0802557468414307\n",
      "Loss: 1.1487451791763306\n",
      "Loss: 1.0121967792510986\n",
      "Loss: 1.0715612173080444\n",
      "Loss: 1.0299768447875977\n",
      "Loss: 1.1733336448669434\n",
      "Loss: 1.074648380279541\n",
      "Loss: 1.078426480293274\n",
      "Loss: 0.9860134124755859\n",
      "Loss: 1.0101827383041382\n",
      "Loss: 1.0624058246612549\n",
      "Loss: 1.0733827352523804\n",
      "Loss: 1.0841748714447021\n",
      "Loss: 1.1212544441223145\n",
      "Loss: 1.0711638927459717\n",
      "Loss: 1.1129162311553955\n",
      "Loss: 1.0791678428649902\n",
      "Loss: 1.0287209749221802\n",
      "Loss: 1.0534896850585938\n",
      "Loss: 1.0551798343658447\n",
      "Loss: 1.0949125289916992\n",
      "Loss: 0.982434093952179\n",
      "Loss: 1.0647555589675903\n",
      "Loss: 1.08395254611969\n",
      "Loss: 1.1177105903625488\n",
      "Loss: 1.0780450105667114\n",
      "Loss: 1.053954005241394\n",
      "Loss: 1.046445608139038\n",
      "Loss: 1.106794834136963\n",
      "Loss: 1.0875117778778076\n",
      "Loss: 1.0744438171386719\n",
      "Loss: 1.0708739757537842\n",
      "Loss: 1.109628677368164\n",
      "Loss: 1.0152368545532227\n",
      "Loss: 1.0534083843231201\n",
      "Loss: 1.0866092443466187\n",
      "Loss: 1.0494135618209839\n",
      "Loss: 1.0525414943695068\n",
      "Loss: 1.1095300912857056\n",
      "Loss: 1.0745158195495605\n",
      "Loss: 1.0624390840530396\n",
      "Loss: 1.1107585430145264\n",
      "Loss: 1.0004262924194336\n",
      "Loss: 1.0205082893371582\n",
      "Loss: 1.039584755897522\n",
      "Loss: 1.030975341796875\n",
      "Loss: 1.0717045068740845\n",
      "Loss: 1.1160576343536377\n",
      "Loss: 1.0058361291885376\n",
      "Loss: 1.1344579458236694\n",
      "Loss: 1.0166813135147095\n",
      "Loss: 1.0231395959854126\n",
      "Loss: 1.1268105506896973\n",
      "Loss: 1.0909568071365356\n",
      "Loss: 1.035810947418213\n",
      "Loss: 1.0427247285842896\n",
      "Loss: 1.1309913396835327\n",
      "Loss: 1.0806541442871094\n",
      "Loss: 1.0842267274856567\n",
      "Loss: 1.102485179901123\n",
      "Loss: 1.0225024223327637\n",
      "Loss: 1.034814715385437\n",
      "Loss: 1.0884416103363037\n",
      "Loss: 1.0485848188400269\n",
      "Loss: 1.114254117012024\n",
      "Loss: 1.0599637031555176\n",
      "Loss: 1.0322009325027466\n",
      "Loss: 1.1319141387939453\n",
      "Loss: 1.1802968978881836\n",
      "Loss: 1.0823296308517456\n",
      "Loss: 1.0815306901931763\n",
      "Loss: 1.0529532432556152\n",
      "Loss: 1.092578411102295\n",
      "Loss: 1.0357564687728882\n",
      "Loss: 1.016603708267212\n",
      "Loss: 1.1400575637817383\n",
      "Loss: 1.0325117111206055\n",
      "Loss: 0.9924931526184082\n",
      "Loss: 1.0179721117019653\n",
      "Loss: 1.0724120140075684\n",
      "Loss: 1.1055697202682495\n",
      "Loss: 1.1260383129119873\n",
      "Loss: 1.1305935382843018\n",
      "Loss: 1.0138229131698608\n",
      "Loss: 0.9711418151855469\n",
      "Loss: 1.1011430025100708\n",
      "Loss: 1.0432422161102295\n",
      "Loss: 1.0757782459259033\n",
      "Loss: 0.9990860223770142\n",
      "Loss: 1.0309809446334839\n",
      "Loss: 1.1005560159683228\n",
      "Loss: 1.065361738204956\n",
      "Loss: 1.0624395608901978\n",
      "Loss: 1.1032906770706177\n",
      "Loss: 1.0987498760223389\n",
      "Loss: 1.036437749862671\n",
      "Loss: 1.070509672164917\n",
      "Loss: 1.0634205341339111\n",
      "Loss: 1.060975432395935\n",
      "Loss: 1.0457667112350464\n",
      "Loss: 1.101322889328003\n",
      "Loss: 0.9685993194580078\n",
      "Loss: 1.026889681816101\n",
      "Loss: 1.0363818407058716\n",
      "Loss: 1.1536811590194702\n",
      "Loss: 1.1205183267593384\n",
      "Loss: 1.1247541904449463\n",
      "Loss: 1.0445297956466675\n",
      "Loss: 1.1460258960723877\n",
      "Loss: 1.108632206916809\n",
      "Loss: 1.124100685119629\n",
      "Loss: 1.0166741609573364\n",
      "Loss: 1.0890448093414307\n",
      "Loss: 1.0660585165023804\n",
      "Loss: 1.0335259437561035\n",
      "Loss: 1.018061876296997\n",
      "Loss: 1.0740991830825806\n",
      "Loss: 1.0832573175430298\n",
      "Loss: 1.019432783126831\n",
      "Loss: 1.0421388149261475\n",
      "Loss: 1.0344667434692383\n",
      "Loss: 1.0899231433868408\n",
      "Loss: 1.0167667865753174\n",
      "Loss: 1.0795930624008179\n",
      "Loss: 1.0670455694198608\n",
      "Loss: 1.0836302042007446\n",
      "Loss: 1.1414251327514648\n",
      "Loss: 1.081662654876709\n",
      "Loss: 1.0633491277694702\n",
      "Loss: 1.0340152978897095\n",
      "Loss: 1.0975221395492554\n",
      "Loss: 1.0916392803192139\n",
      "Loss: 1.0402398109436035\n",
      "Loss: 1.0844601392745972\n",
      "Loss: 1.0973578691482544\n",
      "Loss: 1.0310474634170532\n",
      "Loss: 0.9763779044151306\n",
      "Loss: 1.1204125881195068\n",
      "Loss: 1.075438380241394\n",
      "Loss: 1.0759482383728027\n",
      "Loss: 1.0728418827056885\n",
      "Loss: 1.0810216665267944\n",
      "Loss: 1.1215308904647827\n",
      "-------- \n",
      " Epoch 5: Average Loss: 1.0708379745483398\n",
      "Epoch 7\n",
      "-----------------------\n",
      "Loss: 1.0098317861557007\n",
      "Loss: 1.0634949207305908\n",
      "Loss: 1.1444333791732788\n",
      "Loss: 1.0078498125076294\n",
      "Loss: 1.0603927373886108\n",
      "Loss: 1.0888749361038208\n",
      "Loss: 1.0442800521850586\n",
      "Loss: 1.1179893016815186\n",
      "Loss: 1.0682529211044312\n",
      "Loss: 1.0787080526351929\n",
      "Loss: 1.0910964012145996\n",
      "Loss: 1.0522446632385254\n",
      "Loss: 1.0577415227890015\n",
      "Loss: 1.0341532230377197\n",
      "Loss: 1.0566625595092773\n",
      "Loss: 0.968685507774353\n",
      "Loss: 1.0835504531860352\n",
      "Loss: 1.0180447101593018\n",
      "Loss: 1.08358633518219\n",
      "Loss: 1.0561197996139526\n",
      "Loss: 1.0798979997634888\n",
      "Loss: 1.1021736860275269\n",
      "Loss: 1.124146580696106\n",
      "Loss: 1.1204215288162231\n",
      "Loss: 1.1255074739456177\n",
      "Loss: 1.0706031322479248\n",
      "Loss: 1.006184458732605\n",
      "Loss: 1.0970635414123535\n",
      "Loss: 1.062500238418579\n",
      "Loss: 1.0897120237350464\n",
      "Loss: 1.0727907419204712\n",
      "Loss: 1.0678454637527466\n",
      "Loss: 1.098958134651184\n",
      "Loss: 1.129419207572937\n",
      "Loss: 1.1610145568847656\n",
      "Loss: 1.0446181297302246\n",
      "Loss: 1.0417895317077637\n",
      "Loss: 1.0701828002929688\n",
      "Loss: 1.1090083122253418\n",
      "Loss: 1.0725834369659424\n",
      "Loss: 1.1184319257736206\n",
      "Loss: 0.9780248403549194\n",
      "Loss: 1.0886110067367554\n",
      "Loss: 1.1339671611785889\n",
      "Loss: 1.0841195583343506\n",
      "Loss: 1.1411575078964233\n",
      "Loss: 1.0486574172973633\n",
      "Loss: 1.0656572580337524\n",
      "Loss: 1.142120361328125\n",
      "Loss: 1.012186050415039\n",
      "Loss: 1.087424635887146\n",
      "Loss: 1.0806608200073242\n",
      "Loss: 0.9775254130363464\n",
      "Loss: 1.0788350105285645\n",
      "Loss: 1.0537974834442139\n",
      "Loss: 1.0807167291641235\n",
      "Loss: 1.0611217021942139\n",
      "Loss: 1.1005550622940063\n",
      "Loss: 1.0513073205947876\n",
      "Loss: 1.0498453378677368\n",
      "Loss: 1.1404755115509033\n",
      "Loss: 1.049631953239441\n",
      "Loss: 1.0791665315628052\n",
      "Loss: 1.1070363521575928\n",
      "Loss: 1.0254931449890137\n",
      "Loss: 1.0290166139602661\n",
      "Loss: 1.1541956663131714\n",
      "Loss: 1.071766972541809\n",
      "Loss: 1.1170024871826172\n",
      "Loss: 1.122185230255127\n",
      "Loss: 1.0490446090698242\n",
      "Loss: 1.0399110317230225\n",
      "Loss: 1.1130485534667969\n",
      "Loss: 1.054414987564087\n",
      "Loss: 1.0147813558578491\n",
      "Loss: 1.0461537837982178\n",
      "Loss: 1.1196317672729492\n",
      "Loss: 1.116148591041565\n",
      "Loss: 1.1466327905654907\n",
      "Loss: 1.0341553688049316\n",
      "Loss: 1.0294265747070312\n",
      "Loss: 1.0105856657028198\n",
      "Loss: 1.1231021881103516\n",
      "Loss: 1.0480375289916992\n",
      "Loss: 1.0894794464111328\n",
      "Loss: 1.0715744495391846\n",
      "Loss: 1.0437755584716797\n",
      "Loss: 1.0534900426864624\n",
      "Loss: 1.0179411172866821\n",
      "Loss: 1.0135629177093506\n",
      "Loss: 1.0577688217163086\n",
      "Loss: 1.0004301071166992\n",
      "Loss: 1.0678094625473022\n",
      "Loss: 1.0847612619400024\n",
      "Loss: 1.073872447013855\n",
      "Loss: 1.081658124923706\n",
      "Loss: 1.0527002811431885\n",
      "Loss: 1.1231791973114014\n",
      "Loss: 1.1216306686401367\n",
      "Loss: 1.091172456741333\n",
      "Loss: 1.1291303634643555\n",
      "Loss: 1.0820050239562988\n",
      "Loss: 1.0947614908218384\n",
      "Loss: 1.0710264444351196\n",
      "Loss: 1.0400694608688354\n",
      "Loss: 1.1133859157562256\n",
      "Loss: 1.1821858882904053\n",
      "Loss: 1.0981919765472412\n",
      "Loss: 1.0745270252227783\n",
      "Loss: 1.105777382850647\n",
      "Loss: 1.0616532564163208\n",
      "Loss: 1.0911080837249756\n",
      "Loss: 1.0922454595565796\n",
      "Loss: 1.092392086982727\n",
      "Loss: 1.0724602937698364\n",
      "Loss: 1.0452991724014282\n",
      "Loss: 1.0744438171386719\n",
      "Loss: 1.0781209468841553\n",
      "Loss: 0.9538893699645996\n",
      "Loss: 1.00121009349823\n",
      "Loss: 1.1058472394943237\n",
      "Loss: 1.129391074180603\n",
      "Loss: 1.128826379776001\n",
      "Loss: 1.1122539043426514\n",
      "Loss: 1.0606801509857178\n",
      "Loss: 0.9866843223571777\n",
      "Loss: 1.0219720602035522\n",
      "Loss: 1.0543062686920166\n",
      "Loss: 1.0105327367782593\n",
      "Loss: 1.0791651010513306\n",
      "Loss: 1.017702341079712\n",
      "Loss: 0.9819053411483765\n",
      "Loss: 1.0896389484405518\n",
      "Loss: 1.0655816793441772\n",
      "Loss: 1.1045148372650146\n",
      "Loss: 1.1765873432159424\n",
      "Loss: 1.0651127099990845\n",
      "Loss: 1.0239346027374268\n",
      "Loss: 1.1172785758972168\n",
      "Loss: 0.9973965287208557\n",
      "Loss: 1.1114311218261719\n",
      "Loss: 1.0458006858825684\n",
      "Loss: 1.0553914308547974\n",
      "Loss: 1.0701167583465576\n",
      "Loss: 1.0721102952957153\n",
      "Loss: 1.0280488729476929\n",
      "Loss: 1.0619488954544067\n",
      "Loss: 1.0827101469039917\n",
      "Loss: 1.0612698793411255\n",
      "Loss: 1.0425242185592651\n",
      "Loss: 1.0707836151123047\n",
      "Loss: 1.0256398916244507\n",
      "Loss: 1.0809803009033203\n",
      "Loss: 1.1447089910507202\n",
      "Loss: 1.0681530237197876\n",
      "Loss: 1.0631167888641357\n",
      "Loss: 1.0702756643295288\n",
      "Loss: 1.0695722103118896\n",
      "Loss: 1.0379396677017212\n",
      "Loss: 1.0600831508636475\n",
      "Loss: 1.0945378541946411\n",
      "Loss: 1.0608117580413818\n",
      "Loss: 1.0100570917129517\n",
      "Loss: 0.9395389556884766\n",
      "Loss: 1.1298784017562866\n",
      "Loss: 1.0993999242782593\n",
      "Loss: 1.1446970701217651\n",
      "Loss: 1.1255947351455688\n",
      "Loss: 1.0789694786071777\n",
      "Loss: 1.0337183475494385\n",
      "Loss: 1.1346243619918823\n",
      "Loss: 1.0882623195648193\n",
      "Loss: 1.0300962924957275\n",
      "Loss: 1.052243709564209\n",
      "Loss: 1.087829828262329\n",
      "Loss: 1.0609875917434692\n",
      "Loss: 1.0099362134933472\n",
      "Loss: 1.0796433687210083\n",
      "Loss: 1.1108258962631226\n",
      "Loss: 1.0155153274536133\n",
      "Loss: 1.1062718629837036\n",
      "Loss: 1.0687391757965088\n",
      "Loss: 1.0195403099060059\n",
      "Loss: 1.0943965911865234\n",
      "Loss: 1.0763781070709229\n",
      "Loss: 1.0823476314544678\n",
      "Loss: 1.0336222648620605\n",
      "Loss: 1.0215667486190796\n",
      "Loss: 1.1233484745025635\n",
      "Loss: 1.101789951324463\n",
      "Loss: 1.0701022148132324\n",
      "Loss: 1.030336618423462\n",
      "Loss: 1.0622657537460327\n",
      "Loss: 1.0732909440994263\n",
      "Loss: 1.14301335811615\n",
      "Loss: 1.097354769706726\n",
      "Loss: 1.080365777015686\n",
      "Loss: 1.098699927330017\n",
      "Loss: 1.011499047279358\n",
      "Loss: 1.1087768077850342\n",
      "Loss: 1.0814604759216309\n",
      "Loss: 1.062783122062683\n",
      "Loss: 1.0552189350128174\n",
      "Loss: 0.9988882541656494\n",
      "Loss: 1.0239698886871338\n",
      "Loss: 1.0376681089401245\n",
      "Loss: 1.1372936964035034\n",
      "Loss: 1.0729939937591553\n",
      "Loss: 1.053656816482544\n",
      "Loss: 1.0681426525115967\n",
      "Loss: 1.1712177991867065\n",
      "Loss: 1.084282398223877\n",
      "Loss: 1.0820962190628052\n",
      "Loss: 1.1032867431640625\n",
      "Loss: 1.023048996925354\n",
      "Loss: 1.0927953720092773\n",
      "Loss: 1.1110644340515137\n",
      "Loss: 1.0894076824188232\n",
      "Loss: 1.0244932174682617\n",
      "Loss: 0.9765138626098633\n",
      "Loss: 1.0038318634033203\n",
      "Loss: 1.0379228591918945\n",
      "Loss: 1.0510799884796143\n",
      "Loss: 1.1402437686920166\n",
      "Loss: 1.1413097381591797\n",
      "Loss: 1.1388541460037231\n",
      "Loss: 1.0806677341461182\n",
      "Loss: 1.0152932405471802\n",
      "Loss: 1.073925495147705\n",
      "Loss: 1.0483204126358032\n",
      "Loss: 1.0714855194091797\n",
      "Loss: 1.0188179016113281\n",
      "Loss: 1.0789395570755005\n",
      "Loss: 1.054383397102356\n",
      "Loss: 1.072473406791687\n",
      "Loss: 1.0885118246078491\n",
      "Loss: 1.1238009929656982\n",
      "Loss: 1.0261449813842773\n",
      "Loss: 1.107935905456543\n",
      "Loss: 1.0061460733413696\n",
      "Loss: 1.0298511981964111\n",
      "Loss: 1.050130844116211\n",
      "Loss: 1.0384737253189087\n",
      "Loss: 1.1509934663772583\n",
      "Loss: 1.105172038078308\n",
      "Loss: 1.043702483177185\n",
      "-------- \n",
      " Epoch 6: Average Loss: 1.0708060264587402\n",
      "Epoch 8\n",
      "-----------------------\n",
      "Loss: 1.0893497467041016\n",
      "Loss: 1.0563137531280518\n",
      "Loss: 1.0704236030578613\n",
      "Loss: 1.1072839498519897\n",
      "Loss: 1.053938388824463\n",
      "Loss: 1.1430457830429077\n",
      "Loss: 1.0530301332473755\n",
      "Loss: 1.1047849655151367\n",
      "Loss: 1.0072863101959229\n",
      "Loss: 1.0493587255477905\n",
      "Loss: 1.0990679264068604\n",
      "Loss: 1.148038625717163\n",
      "Loss: 1.03593909740448\n",
      "Loss: 1.0818920135498047\n",
      "Loss: 1.0446903705596924\n",
      "Loss: 0.9886970520019531\n",
      "Loss: 1.028740406036377\n",
      "Loss: 0.993833065032959\n",
      "Loss: 1.0803078413009644\n",
      "Loss: 1.093969702720642\n",
      "Loss: 1.003978967666626\n",
      "Loss: 1.123338222503662\n",
      "Loss: 1.0531994104385376\n",
      "Loss: 1.1281609535217285\n",
      "Loss: 1.0858112573623657\n",
      "Loss: 0.9876716732978821\n",
      "Loss: 1.0508270263671875\n",
      "Loss: 1.13111412525177\n",
      "Loss: 1.1245229244232178\n",
      "Loss: 1.10645592212677\n",
      "Loss: 1.0988904237747192\n",
      "Loss: 1.0525928735733032\n",
      "Loss: 1.0838409662246704\n",
      "Loss: 1.0661418437957764\n",
      "Loss: 1.0895836353302002\n",
      "Loss: 1.0090198516845703\n",
      "Loss: 1.0608586072921753\n",
      "Loss: 1.0050396919250488\n",
      "Loss: 1.0997989177703857\n",
      "Loss: 1.0036684274673462\n",
      "Loss: 1.1025556325912476\n",
      "Loss: 0.962777853012085\n",
      "Loss: 1.083538293838501\n",
      "Loss: 1.0998826026916504\n",
      "Loss: 1.1005445718765259\n",
      "Loss: 1.1231809854507446\n",
      "Loss: 1.0959625244140625\n",
      "Loss: 1.0791590213775635\n",
      "Loss: 1.04388427734375\n",
      "Loss: 1.1103111505508423\n",
      "Loss: 1.1242541074752808\n",
      "Loss: 1.0535627603530884\n",
      "Loss: 1.0483042001724243\n",
      "Loss: 1.0355464220046997\n",
      "Loss: 1.0982493162155151\n",
      "Loss: 1.0316020250320435\n",
      "Loss: 1.0403504371643066\n",
      "Loss: 1.0787944793701172\n",
      "Loss: 1.0716880559921265\n",
      "Loss: 1.0057538747787476\n",
      "Loss: 1.087048888206482\n",
      "Loss: 1.0753915309906006\n",
      "Loss: 1.0331530570983887\n",
      "Loss: 1.0425101518630981\n",
      "Loss: 1.099352478981018\n",
      "Loss: 1.1106492280960083\n",
      "Loss: 1.095625400543213\n",
      "Loss: 1.0932536125183105\n",
      "Loss: 1.1229935884475708\n",
      "Loss: 1.099381923675537\n",
      "Loss: 1.0561810731887817\n",
      "Loss: 0.9865226149559021\n",
      "Loss: 1.1389169692993164\n",
      "Loss: 1.0533525943756104\n",
      "Loss: 1.0167670249938965\n",
      "Loss: 1.129170298576355\n",
      "Loss: 1.0955268144607544\n",
      "Loss: 1.0874627828598022\n",
      "Loss: 0.9965446591377258\n",
      "Loss: 1.1265567541122437\n",
      "Loss: 1.0699803829193115\n",
      "Loss: 1.0344066619873047\n",
      "Loss: 1.1138536930084229\n",
      "Loss: 1.0977507829666138\n",
      "Loss: 1.0747015476226807\n",
      "Loss: 1.1373096704483032\n",
      "Loss: 1.121552586555481\n",
      "Loss: 1.0289205312728882\n",
      "Loss: 1.009167194366455\n",
      "Loss: 1.0445917844772339\n",
      "Loss: 1.0797961950302124\n",
      "Loss: 1.1120855808258057\n",
      "Loss: 1.1043826341629028\n",
      "Loss: 1.03110933303833\n",
      "Loss: 1.0518983602523804\n",
      "Loss: 1.0468840599060059\n",
      "Loss: 1.105678677558899\n",
      "Loss: 1.0784786939620972\n",
      "Loss: 1.101257562637329\n",
      "Loss: 1.160733938217163\n",
      "Loss: 1.0641289949417114\n",
      "Loss: 1.020708680152893\n",
      "Loss: 1.0713051557540894\n",
      "Loss: 1.087768316268921\n",
      "Loss: 1.122926950454712\n",
      "Loss: 1.1197549104690552\n",
      "Loss: 1.073203444480896\n",
      "Loss: 1.0611722469329834\n",
      "Loss: 1.0950638055801392\n",
      "Loss: 1.0917447805404663\n",
      "Loss: 1.0808569192886353\n",
      "Loss: 1.0820037126541138\n",
      "Loss: 1.0584990978240967\n",
      "Loss: 1.1486486196517944\n",
      "Loss: 1.1094889640808105\n",
      "Loss: 1.07191002368927\n",
      "Loss: 0.9886322021484375\n",
      "Loss: 0.9739681482315063\n",
      "Loss: 1.0331684350967407\n",
      "Loss: 1.147963285446167\n",
      "Loss: 1.0603642463684082\n",
      "Loss: 1.118430733680725\n",
      "Loss: 1.0461888313293457\n",
      "Loss: 1.082173466682434\n",
      "Loss: 1.0983015298843384\n",
      "Loss: 1.0820672512054443\n",
      "Loss: 1.0940656661987305\n",
      "Loss: 1.0585142374038696\n",
      "Loss: 1.0706470012664795\n",
      "Loss: 1.0694752931594849\n",
      "Loss: 1.0533111095428467\n",
      "Loss: 1.023146152496338\n",
      "Loss: 1.0273836851119995\n",
      "Loss: 1.0095974206924438\n",
      "Loss: 1.071120262145996\n",
      "Loss: 0.9705057740211487\n",
      "Loss: 1.0647125244140625\n",
      "Loss: 1.0211760997772217\n",
      "Loss: 1.1946330070495605\n",
      "Loss: 1.031625747680664\n",
      "Loss: 1.026202917098999\n",
      "Loss: 0.9974207282066345\n",
      "Loss: 1.0492805242538452\n",
      "Loss: 1.102077603340149\n",
      "Loss: 1.0639630556106567\n",
      "Loss: 1.0875473022460938\n",
      "Loss: 1.0145162343978882\n",
      "Loss: 1.1620442867279053\n",
      "Loss: 1.0089874267578125\n",
      "Loss: 1.128121256828308\n",
      "Loss: 1.0253843069076538\n",
      "Loss: 1.0912185907363892\n",
      "Loss: 1.1201221942901611\n",
      "Loss: 1.0996263027191162\n",
      "Loss: 1.0510538816452026\n",
      "Loss: 1.061828851699829\n",
      "Loss: 1.1254843473434448\n",
      "Loss: 1.0892715454101562\n",
      "Loss: 1.0933794975280762\n",
      "Loss: 1.173114538192749\n",
      "Loss: 1.0242745876312256\n",
      "Loss: 1.0588425397872925\n",
      "Loss: 1.0279241800308228\n",
      "Loss: 1.0823156833648682\n",
      "Loss: 1.0720826387405396\n",
      "Loss: 1.0546400547027588\n",
      "Loss: 1.037306785583496\n",
      "Loss: 1.0904347896575928\n",
      "Loss: 1.1100198030471802\n",
      "Loss: 1.01667058467865\n",
      "Loss: 1.0968241691589355\n",
      "Loss: 1.1129419803619385\n",
      "Loss: 1.124009609222412\n",
      "Loss: 1.0638442039489746\n",
      "Loss: 1.0340816974639893\n",
      "Loss: 1.147420048713684\n",
      "Loss: 1.0923100709915161\n",
      "Loss: 1.1373542547225952\n",
      "Loss: 1.054730772972107\n",
      "Loss: 1.100585699081421\n",
      "Loss: 0.9849291443824768\n",
      "Loss: 1.1064671277999878\n",
      "Loss: 1.0878421068191528\n",
      "Loss: 1.0738574266433716\n",
      "Loss: 1.0351334810256958\n",
      "Loss: 1.103590726852417\n",
      "Loss: 1.0799223184585571\n",
      "Loss: 1.0674721002578735\n",
      "Loss: 1.0733489990234375\n",
      "Loss: 1.063140869140625\n",
      "Loss: 1.0731277465820312\n",
      "Loss: 1.0790427923202515\n",
      "Loss: 1.0602353811264038\n",
      "Loss: 1.0897998809814453\n",
      "Loss: 1.0454742908477783\n",
      "Loss: 1.02625572681427\n",
      "Loss: 1.1006221771240234\n",
      "Loss: 1.0423516035079956\n",
      "Loss: 1.06282639503479\n",
      "Loss: 1.0730267763137817\n",
      "Loss: 1.0870908498764038\n",
      "Loss: 1.0600197315216064\n",
      "Loss: 1.1072616577148438\n",
      "Loss: 1.0577783584594727\n",
      "Loss: 1.0280448198318481\n",
      "Loss: 1.101097822189331\n",
      "Loss: 1.117073655128479\n",
      "Loss: 1.1306589841842651\n",
      "Loss: 1.0150790214538574\n",
      "Loss: 1.0823173522949219\n",
      "Loss: 1.0843652486801147\n",
      "Loss: 1.0473015308380127\n",
      "Loss: 1.072019338607788\n",
      "Loss: 1.082250952720642\n",
      "Loss: 1.0632096529006958\n",
      "Loss: 1.1193739175796509\n",
      "Loss: 1.0428482294082642\n",
      "Loss: 0.9974398612976074\n",
      "Loss: 1.0264099836349487\n",
      "Loss: 1.039597988128662\n",
      "Loss: 1.0927953720092773\n",
      "Loss: 1.0481280088424683\n",
      "Loss: 1.080749750137329\n",
      "Loss: 1.09196937084198\n",
      "Loss: 1.1836360692977905\n",
      "Loss: 1.0190582275390625\n",
      "Loss: 1.0221846103668213\n",
      "Loss: 1.0901778936386108\n",
      "Loss: 1.0970197916030884\n",
      "Loss: 1.1006200313568115\n",
      "Loss: 1.1005054712295532\n",
      "Loss: 1.1224634647369385\n",
      "Loss: 1.090783715248108\n",
      "Loss: 1.1036056280136108\n",
      "Loss: 1.05388343334198\n",
      "Loss: 1.0634790658950806\n",
      "Loss: 1.0179868936538696\n",
      "Loss: 1.0730518102645874\n",
      "Loss: 1.1050511598587036\n",
      "Loss: 1.0437095165252686\n",
      "Loss: 1.0426980257034302\n",
      "Loss: 1.0585390329360962\n",
      "Loss: 1.048026204109192\n",
      "Loss: 1.079450011253357\n",
      "Loss: 1.0273115634918213\n",
      "Loss: 1.065233588218689\n",
      "-------- \n",
      " Epoch 7: Average Loss: 1.070709228515625\n",
      "Epoch 9\n",
      "-----------------------\n",
      "Loss: 1.0717496871948242\n",
      "Loss: 1.0763485431671143\n",
      "Loss: 1.1182318925857544\n",
      "Loss: 1.0994714498519897\n",
      "Loss: 1.0348412990570068\n",
      "Loss: 1.1601219177246094\n",
      "Loss: 1.0539671182632446\n",
      "Loss: 1.0699487924575806\n",
      "Loss: 1.1231087446212769\n",
      "Loss: 1.0963588953018188\n",
      "Loss: 1.071173906326294\n",
      "Loss: 1.0361093282699585\n",
      "Loss: 1.1028077602386475\n",
      "Loss: 1.1306581497192383\n",
      "Loss: 1.1278830766677856\n",
      "Loss: 1.021153450012207\n",
      "Loss: 1.0300734043121338\n",
      "Loss: 1.111396074295044\n",
      "Loss: 1.03392493724823\n",
      "Loss: 1.0977685451507568\n",
      "Loss: 1.0696797370910645\n",
      "Loss: 1.0825533866882324\n",
      "Loss: 1.0905940532684326\n",
      "Loss: 1.0812842845916748\n",
      "Loss: 1.0901684761047363\n",
      "Loss: 1.1323665380477905\n",
      "Loss: 1.045237421989441\n",
      "Loss: 1.080693244934082\n",
      "Loss: 1.0893011093139648\n",
      "Loss: 1.064103126525879\n",
      "Loss: 1.0674030780792236\n",
      "Loss: 1.0761868953704834\n",
      "Loss: 1.1006256341934204\n",
      "Loss: 1.0256969928741455\n",
      "Loss: 1.0351637601852417\n",
      "Loss: 1.0754433870315552\n",
      "Loss: 1.0907589197158813\n",
      "Loss: 1.1121058464050293\n",
      "Loss: 1.1270081996917725\n",
      "Loss: 1.0530604124069214\n",
      "Loss: 1.0412201881408691\n",
      "Loss: 1.035666823387146\n",
      "Loss: 1.0835590362548828\n",
      "Loss: 1.12948739528656\n",
      "Loss: 1.1282143592834473\n",
      "Loss: 1.0348942279815674\n",
      "Loss: 1.0131230354309082\n",
      "Loss: 1.0902674198150635\n",
      "Loss: 1.068239688873291\n",
      "Loss: 1.0628125667572021\n",
      "Loss: 1.0763522386550903\n",
      "Loss: 1.1810121536254883\n",
      "Loss: 1.0001640319824219\n",
      "Loss: 1.0809673070907593\n",
      "Loss: 1.0235974788665771\n",
      "Loss: 1.0980561971664429\n",
      "Loss: 1.146923542022705\n",
      "Loss: 0.9895892143249512\n",
      "Loss: 1.1361253261566162\n",
      "Loss: 1.0501468181610107\n",
      "Loss: 1.0628596544265747\n",
      "Loss: 1.0804269313812256\n",
      "Loss: 1.0829962491989136\n",
      "Loss: 1.0386086702346802\n",
      "Loss: 1.0359028577804565\n",
      "Loss: 1.0437264442443848\n",
      "Loss: 1.0446563959121704\n",
      "Loss: 1.006162405014038\n",
      "Loss: 1.053879737854004\n",
      "Loss: 1.0437488555908203\n",
      "Loss: 1.1022652387619019\n",
      "Loss: 1.0546133518218994\n",
      "Loss: 1.1551510095596313\n",
      "Loss: 1.0346077680587769\n",
      "Loss: 1.0787901878356934\n",
      "Loss: 1.0335912704467773\n",
      "Loss: 1.1267584562301636\n",
      "Loss: 1.083577275276184\n",
      "Loss: 1.0997248888015747\n",
      "Loss: 1.1268068552017212\n",
      "Loss: 1.0767641067504883\n",
      "Loss: 1.0847142934799194\n",
      "Loss: 1.0454270839691162\n",
      "Loss: 1.0703164339065552\n",
      "Loss: 1.1221662759780884\n",
      "Loss: 1.037124514579773\n",
      "Loss: 1.0742326974868774\n",
      "Loss: 1.1026643514633179\n",
      "Loss: 1.0806959867477417\n",
      "Loss: 1.0647821426391602\n",
      "Loss: 1.0687620639801025\n",
      "Loss: 1.077638864517212\n",
      "Loss: 1.0279823541641235\n",
      "Loss: 1.042820930480957\n",
      "Loss: 1.0095603466033936\n",
      "Loss: 1.1279499530792236\n",
      "Loss: 1.1259486675262451\n",
      "Loss: 1.0896662473678589\n",
      "Loss: 1.0209847688674927\n",
      "Loss: 1.0629929304122925\n",
      "Loss: 1.099815011024475\n",
      "Loss: 1.0649946928024292\n",
      "Loss: 1.0281227827072144\n",
      "Loss: 1.1299971342086792\n",
      "Loss: 1.0478792190551758\n",
      "Loss: 1.073316216468811\n",
      "Loss: 1.106127142906189\n",
      "Loss: 1.1266975402832031\n",
      "Loss: 1.0263521671295166\n",
      "Loss: 1.0826780796051025\n",
      "Loss: 1.0892044305801392\n",
      "Loss: 1.0364406108856201\n",
      "Loss: 1.0706642866134644\n",
      "Loss: 1.010474681854248\n",
      "Loss: 1.0246036052703857\n",
      "Loss: 1.0737608671188354\n",
      "Loss: 0.9711067080497742\n",
      "Loss: 1.0628550052642822\n",
      "Loss: 1.0793918371200562\n",
      "Loss: 1.1301220655441284\n",
      "Loss: 1.0574358701705933\n",
      "Loss: 1.1021814346313477\n",
      "Loss: 1.035111427307129\n",
      "Loss: 1.1193640232086182\n",
      "Loss: 1.1368404626846313\n",
      "Loss: 1.0738017559051514\n",
      "Loss: 1.1534019708633423\n",
      "Loss: 1.1024893522262573\n",
      "Loss: 1.1107258796691895\n",
      "Loss: 1.1000423431396484\n",
      "Loss: 1.0598015785217285\n",
      "Loss: 1.040266990661621\n",
      "Loss: 1.1004618406295776\n",
      "Loss: 1.0378316640853882\n",
      "Loss: 1.0006729364395142\n",
      "Loss: 0.9938046932220459\n",
      "Loss: 1.1292940378189087\n",
      "Loss: 1.2019282579421997\n",
      "Loss: 1.119794249534607\n",
      "Loss: 1.0309635400772095\n",
      "Loss: 1.1444268226623535\n",
      "Loss: 1.1165968179702759\n",
      "Loss: 0.9982378482818604\n",
      "Loss: 1.0447564125061035\n",
      "Loss: 1.0625938177108765\n",
      "Loss: 1.1024532318115234\n",
      "Loss: 0.9927451014518738\n",
      "Loss: 1.076138973236084\n",
      "Loss: 1.0231232643127441\n",
      "Loss: 1.0828180313110352\n",
      "Loss: 1.0609999895095825\n",
      "Loss: 1.080641508102417\n",
      "Loss: 1.050513505935669\n",
      "Loss: 0.9903579354286194\n",
      "Loss: 1.0812321901321411\n",
      "Loss: 1.0748134851455688\n",
      "Loss: 1.0815297365188599\n",
      "Loss: 1.0794329643249512\n",
      "Loss: 1.090649962425232\n",
      "Loss: 1.1436251401901245\n",
      "Loss: 1.0889132022857666\n",
      "Loss: 1.0170475244522095\n",
      "Loss: 1.0736411809921265\n",
      "Loss: 1.1136620044708252\n",
      "Loss: 1.007472276687622\n",
      "Loss: 1.132554292678833\n",
      "Loss: 1.071121096611023\n",
      "Loss: 1.1174709796905518\n",
      "Loss: 1.0537283420562744\n",
      "Loss: 1.1181696653366089\n",
      "Loss: 1.1094818115234375\n",
      "Loss: 1.0452815294265747\n",
      "Loss: 0.9789576530456543\n",
      "Loss: 1.0133332014083862\n",
      "Loss: 1.0614629983901978\n",
      "Loss: 1.0987608432769775\n",
      "Loss: 1.0722676515579224\n",
      "Loss: 1.033534288406372\n",
      "Loss: 1.063004970550537\n",
      "Loss: 0.9983049035072327\n",
      "Loss: 1.1576178073883057\n",
      "Loss: 1.0895262956619263\n",
      "Loss: 1.0323646068572998\n",
      "Loss: 1.0253020524978638\n",
      "Loss: 1.0972065925598145\n",
      "Loss: 1.0459699630737305\n",
      "Loss: 1.1029869318008423\n",
      "Loss: 1.13494074344635\n",
      "Loss: 1.1256747245788574\n",
      "Loss: 1.0181987285614014\n",
      "Loss: 1.0899348258972168\n",
      "Loss: 1.0657063722610474\n",
      "Loss: 1.140316128730774\n",
      "Loss: 1.0442144870758057\n",
      "Loss: 1.062526822090149\n",
      "Loss: 1.0421485900878906\n",
      "Loss: 1.0728368759155273\n",
      "Loss: 1.1757923364639282\n",
      "Loss: 1.1019269227981567\n",
      "Loss: 1.0622069835662842\n",
      "Loss: 1.0702013969421387\n",
      "Loss: 0.9573559761047363\n",
      "Loss: 1.0154013633728027\n",
      "Loss: 1.0365914106369019\n",
      "Loss: 1.1169565916061401\n",
      "Loss: 1.0988346338272095\n",
      "Loss: 1.0992350578308105\n",
      "Loss: 1.0445557832717896\n",
      "Loss: 1.073133111000061\n",
      "Loss: 1.1001774072647095\n",
      "Loss: 1.0760148763656616\n",
      "Loss: 1.0564159154891968\n",
      "Loss: 1.0376694202423096\n",
      "Loss: 1.136534333229065\n",
      "Loss: 1.0670645236968994\n",
      "Loss: 1.1242482662200928\n",
      "Loss: 1.1349433660507202\n",
      "Loss: 1.066807508468628\n",
      "Loss: 1.0804598331451416\n",
      "Loss: 1.128587245941162\n",
      "Loss: 1.1219528913497925\n",
      "Loss: 1.0152184963226318\n",
      "Loss: 1.0963890552520752\n",
      "Loss: 1.0430678129196167\n",
      "Loss: 1.0276566743850708\n",
      "Loss: 1.0892993211746216\n",
      "Loss: 1.104790210723877\n",
      "Loss: 1.053714394569397\n",
      "Loss: 1.10832679271698\n",
      "Loss: 1.0731812715530396\n",
      "Loss: 1.060681700706482\n",
      "Loss: 1.1010018587112427\n",
      "Loss: 0.9805890917778015\n",
      "Loss: 1.1626777648925781\n",
      "Loss: 0.9798567891120911\n",
      "Loss: 1.124326467514038\n",
      "Loss: 1.1073747873306274\n",
      "Loss: 1.0342849493026733\n",
      "Loss: 1.1308653354644775\n",
      "Loss: 1.0572736263275146\n",
      "Loss: 1.0916132926940918\n",
      "Loss: 1.034485101699829\n",
      "Loss: 1.1253933906555176\n",
      "Loss: 1.0415235757827759\n",
      "Loss: 1.04527747631073\n",
      "Loss: 1.130097508430481\n",
      "-------- \n",
      " Epoch 8: Average Loss: 1.0706425905227661\n",
      "Epoch 10\n",
      "-----------------------\n",
      "Loss: 1.089582920074463\n",
      "Loss: 1.0971400737762451\n",
      "Loss: 1.0831395387649536\n",
      "Loss: 1.1070804595947266\n",
      "Loss: 1.1125853061676025\n",
      "Loss: 1.0565078258514404\n",
      "Loss: 1.072361946105957\n",
      "Loss: 1.1196379661560059\n",
      "Loss: 1.1164559125900269\n",
      "Loss: 1.0465456247329712\n",
      "Loss: 1.0359009504318237\n",
      "Loss: 1.0856244564056396\n",
      "Loss: 1.0537497997283936\n",
      "Loss: 1.0984809398651123\n",
      "Loss: 1.0802597999572754\n",
      "Loss: 1.0868580341339111\n",
      "Loss: 1.039019227027893\n",
      "Loss: 1.0538160800933838\n",
      "Loss: 1.0867149829864502\n",
      "Loss: 1.065599799156189\n",
      "Loss: 1.072589635848999\n",
      "Loss: 1.0618654489517212\n",
      "Loss: 1.0390573740005493\n",
      "Loss: 1.0792561769485474\n",
      "Loss: 1.1356757879257202\n",
      "Loss: 1.1231831312179565\n",
      "Loss: 1.0351362228393555\n",
      "Loss: 1.044880986213684\n",
      "Loss: 1.2366148233413696\n",
      "Loss: 1.025447130203247\n",
      "Loss: 1.0515259504318237\n",
      "Loss: 1.103671908378601\n",
      "Loss: 1.100072979927063\n",
      "Loss: 1.0653842687606812\n",
      "Loss: 1.0315983295440674\n",
      "Loss: 1.1024380922317505\n",
      "Loss: 1.076952338218689\n",
      "Loss: 0.9712754487991333\n",
      "Loss: 1.0721901655197144\n",
      "Loss: 1.0549672842025757\n",
      "Loss: 0.9687309265136719\n",
      "Loss: 1.0897011756896973\n",
      "Loss: 1.0152333974838257\n",
      "Loss: 1.0736786127090454\n",
      "Loss: 1.1058549880981445\n",
      "Loss: 1.0137759447097778\n",
      "Loss: 1.0416924953460693\n",
      "Loss: 1.070785641670227\n",
      "Loss: 1.0705801248550415\n",
      "Loss: 1.063790202140808\n",
      "Loss: 1.0052284002304077\n",
      "Loss: 1.1294834613800049\n",
      "Loss: 1.092124104499817\n",
      "Loss: 1.1355788707733154\n",
      "Loss: 1.0092062950134277\n",
      "Loss: 1.0901111364364624\n",
      "Loss: 1.0980292558670044\n",
      "Loss: 1.0538575649261475\n",
      "Loss: 1.186259388923645\n",
      "Loss: 1.0433746576309204\n",
      "Loss: 1.0729957818984985\n",
      "Loss: 1.0996429920196533\n",
      "Loss: 1.0470857620239258\n",
      "Loss: 1.0354197025299072\n",
      "Loss: 1.1072436571121216\n",
      "Loss: 1.047511339187622\n",
      "Loss: 1.1098700761795044\n",
      "Loss: 1.0652410984039307\n",
      "Loss: 1.0907399654388428\n",
      "Loss: 1.1072940826416016\n",
      "Loss: 1.071411371231079\n",
      "Loss: 0.9915285110473633\n",
      "Loss: 1.035306453704834\n",
      "Loss: 1.1572744846343994\n",
      "Loss: 1.0633327960968018\n",
      "Loss: 1.0204898118972778\n",
      "Loss: 1.140974760055542\n",
      "Loss: 1.0529054403305054\n",
      "Loss: 1.1160268783569336\n",
      "Loss: 1.0725866556167603\n",
      "Loss: 0.9807915091514587\n",
      "Loss: 1.0712522268295288\n",
      "Loss: 1.0561552047729492\n",
      "Loss: 1.0644279718399048\n",
      "Loss: 1.073410153388977\n",
      "Loss: 1.089055061340332\n",
      "Loss: 1.0524029731750488\n",
      "Loss: 1.0616568326950073\n",
      "Loss: 1.113626480102539\n",
      "Loss: 1.1167211532592773\n",
      "Loss: 1.0988057851791382\n",
      "Loss: 1.025164008140564\n",
      "Loss: 1.1088258028030396\n",
      "Loss: 1.054129958152771\n",
      "Loss: 1.0560402870178223\n",
      "Loss: 1.054053544998169\n",
      "Loss: 1.037174940109253\n",
      "Loss: 1.0543928146362305\n",
      "Loss: 1.0866880416870117\n",
      "Loss: 1.0872303247451782\n",
      "Loss: 1.0920019149780273\n",
      "Loss: 1.0823962688446045\n",
      "Loss: 1.1645079851150513\n",
      "Loss: 1.0884573459625244\n",
      "Loss: 1.1481807231903076\n",
      "Loss: 1.0626230239868164\n",
      "Loss: 1.0696641206741333\n",
      "Loss: 1.1082512140274048\n",
      "Loss: 1.1558393239974976\n",
      "Loss: 1.061381220817566\n",
      "Loss: 1.0721626281738281\n",
      "Loss: 1.0722533464431763\n",
      "Loss: 1.0454803705215454\n",
      "Loss: 0.9650489687919617\n",
      "Loss: 0.9600279331207275\n",
      "Loss: 1.0874464511871338\n",
      "Loss: 1.0441426038742065\n",
      "Loss: 1.0149519443511963\n",
      "Loss: 1.0796102285385132\n",
      "Loss: 1.1220533847808838\n",
      "Loss: 1.0821458101272583\n",
      "Loss: 1.0912063121795654\n",
      "Loss: 1.113809585571289\n",
      "Loss: 1.0799676179885864\n",
      "Loss: 1.142683744430542\n",
      "Loss: 1.0855251550674438\n",
      "Loss: 0.9992238283157349\n",
      "Loss: 1.0850533246994019\n",
      "Loss: 1.134329915046692\n",
      "Loss: 1.111087441444397\n",
      "Loss: 1.0144275426864624\n",
      "Loss: 1.1231399774551392\n",
      "Loss: 1.0725773572921753\n",
      "Loss: 1.098443627357483\n",
      "Loss: 1.0719319581985474\n",
      "Loss: 1.01678466796875\n",
      "Loss: 1.0885074138641357\n",
      "Loss: 1.0151219367980957\n",
      "Loss: 1.1484613418579102\n",
      "Loss: 1.0442659854888916\n",
      "Loss: 1.074011206626892\n",
      "Loss: 1.0801661014556885\n",
      "Loss: 1.1167960166931152\n",
      "Loss: 1.0861843824386597\n",
      "Loss: 1.0645760297775269\n",
      "Loss: 1.0392590761184692\n",
      "Loss: 1.0680269002914429\n",
      "Loss: 1.1179479360580444\n",
      "Loss: 1.0162745714187622\n",
      "Loss: 1.0370330810546875\n",
      "Loss: 1.0522242784500122\n",
      "Loss: 1.1663281917572021\n",
      "Loss: 1.0814450979232788\n",
      "Loss: 1.047178030014038\n",
      "Loss: 1.0153533220291138\n",
      "Loss: 1.0723007917404175\n",
      "Loss: 1.1704049110412598\n",
      "Loss: 1.0143955945968628\n",
      "Loss: 1.0574043989181519\n",
      "Loss: 1.082299828529358\n",
      "Loss: 1.0765119791030884\n",
      "Loss: 1.1379457712173462\n",
      "Loss: 1.0880157947540283\n",
      "Loss: 1.1336644887924194\n",
      "Loss: 1.0242478847503662\n",
      "Loss: 1.0898867845535278\n",
      "Loss: 1.0686062574386597\n",
      "Loss: 1.058899164199829\n",
      "Loss: 1.022979736328125\n",
      "Loss: 1.0649324655532837\n",
      "Loss: 1.0623799562454224\n",
      "Loss: 1.0526256561279297\n",
      "Loss: 1.0484700202941895\n",
      "Loss: 1.0297794342041016\n",
      "Loss: 1.109955072402954\n",
      "Loss: 1.015212893486023\n",
      "Loss: 1.1150684356689453\n",
      "Loss: 1.039965033531189\n",
      "Loss: 1.0071419477462769\n",
      "Loss: 1.0530263185501099\n",
      "Loss: 0.9893879294395447\n",
      "Loss: 1.1359820365905762\n",
      "Loss: 1.1015686988830566\n",
      "Loss: 1.0736249685287476\n",
      "Loss: 1.140183687210083\n",
      "Loss: 1.1167702674865723\n",
      "Loss: 1.018188714981079\n",
      "Loss: 1.1031724214553833\n",
      "Loss: 1.0712898969650269\n",
      "Loss: 1.1473355293273926\n",
      "Loss: 1.1004594564437866\n",
      "Loss: 1.104507327079773\n",
      "Loss: 1.1350423097610474\n",
      "Loss: 1.0514140129089355\n",
      "Loss: 1.0964837074279785\n",
      "Loss: 1.0814509391784668\n",
      "Loss: 1.061640977859497\n",
      "Loss: 1.0732601881027222\n",
      "Loss: 1.0814988613128662\n",
      "Loss: 1.1358498334884644\n",
      "Loss: 1.0185753107070923\n",
      "Loss: 1.0537389516830444\n",
      "Loss: 1.05539870262146\n",
      "Loss: 1.1196324825286865\n",
      "Loss: 1.0626356601715088\n",
      "Loss: 1.0633244514465332\n",
      "Loss: 1.0377583503723145\n",
      "Loss: 1.0449883937835693\n",
      "Loss: 1.0478755235671997\n",
      "Loss: 1.1353005170822144\n",
      "Loss: 1.054923415184021\n",
      "Loss: 1.0625102519989014\n",
      "Loss: 1.0543526411056519\n",
      "Loss: 1.015960454940796\n",
      "Loss: 1.0995670557022095\n",
      "Loss: 1.143985390663147\n",
      "Loss: 1.0816813707351685\n",
      "Loss: 1.0907200574874878\n",
      "Loss: 1.0602200031280518\n",
      "Loss: 1.110442876815796\n",
      "Loss: 1.0947715044021606\n",
      "Loss: 1.1328896284103394\n",
      "Loss: 1.05426824092865\n",
      "Loss: 0.9544975757598877\n",
      "Loss: 1.0941085815429688\n",
      "Loss: 1.11695396900177\n",
      "Loss: 1.0371649265289307\n",
      "Loss: 1.0931743383407593\n",
      "Loss: 1.0803638696670532\n",
      "Loss: 0.994535505771637\n",
      "Loss: 1.0083762407302856\n",
      "Loss: 1.046713948249817\n",
      "Loss: 1.0240358114242554\n",
      "Loss: 1.0432535409927368\n",
      "Loss: 1.063913106918335\n",
      "Loss: 1.1413601636886597\n",
      "Loss: 1.1550028324127197\n",
      "Loss: 1.071632742881775\n",
      "Loss: 1.0886262655258179\n",
      "Loss: 1.0250436067581177\n",
      "Loss: 1.034410834312439\n",
      "Loss: 1.0608755350112915\n",
      "Loss: 1.1072229146957397\n",
      "Loss: 1.130487322807312\n",
      "Loss: 1.106810450553894\n",
      "Loss: 1.0533835887908936\n",
      "-------- \n",
      " Epoch 9: Average Loss: 1.0705680847167969\n",
      "Epoch 11\n",
      "-----------------------\n",
      "Loss: 1.0204828977584839\n",
      "Loss: 1.1313278675079346\n",
      "Loss: 1.0916087627410889\n",
      "Loss: 0.9976785182952881\n",
      "Loss: 1.0552356243133545\n",
      "Loss: 1.0784152746200562\n",
      "Loss: 1.0484756231307983\n",
      "Loss: 1.0614945888519287\n",
      "Loss: 1.026066780090332\n",
      "Loss: 1.046944499015808\n",
      "Loss: 1.062540888786316\n",
      "Loss: 1.0635985136032104\n",
      "Loss: 1.0942354202270508\n",
      "Loss: 1.0615133047103882\n",
      "Loss: 1.0443954467773438\n",
      "Loss: 1.0114585161209106\n",
      "Loss: 0.9650485515594482\n",
      "Loss: 1.0783969163894653\n",
      "Loss: 1.070077657699585\n",
      "Loss: 1.08231782913208\n",
      "Loss: 1.0454870462417603\n",
      "Loss: 1.0644851922988892\n",
      "Loss: 1.1142922639846802\n",
      "Loss: 1.087968349456787\n",
      "Loss: 1.1763291358947754\n",
      "Loss: 1.0876230001449585\n",
      "Loss: 1.014167308807373\n",
      "Loss: 1.1287357807159424\n",
      "Loss: 1.110909342765808\n",
      "Loss: 1.0904455184936523\n",
      "Loss: 1.0714069604873657\n",
      "Loss: 1.0723950862884521\n",
      "Loss: 1.1068819761276245\n",
      "Loss: 1.10721755027771\n",
      "Loss: 1.0619654655456543\n",
      "Loss: 1.1577117443084717\n",
      "Loss: 1.072493553161621\n",
      "Loss: 1.0926175117492676\n",
      "Loss: 1.0705143213272095\n",
      "Loss: 1.1405885219573975\n",
      "Loss: 1.0536340475082397\n",
      "Loss: 1.1223441362380981\n",
      "Loss: 1.0636591911315918\n",
      "Loss: 1.0486862659454346\n",
      "Loss: 1.1280962228775024\n",
      "Loss: 1.016695261001587\n",
      "Loss: 1.0895898342132568\n",
      "Loss: 1.0747770071029663\n",
      "Loss: 1.0808972120285034\n",
      "Loss: 1.044533371925354\n",
      "Loss: 1.1305046081542969\n",
      "Loss: 1.080733060836792\n",
      "Loss: 1.0350444316864014\n",
      "Loss: 1.0366650819778442\n",
      "Loss: 1.044513463973999\n",
      "Loss: 1.052655577659607\n",
      "Loss: 1.1324833631515503\n",
      "Loss: 0.9896191358566284\n",
      "Loss: 1.0180869102478027\n",
      "Loss: 1.0513389110565186\n",
      "Loss: 1.03822660446167\n",
      "Loss: 1.0906224250793457\n",
      "Loss: 0.9978256225585938\n",
      "Loss: 1.0015885829925537\n",
      "Loss: 1.0727782249450684\n",
      "Loss: 1.0991857051849365\n",
      "Loss: 1.0542628765106201\n",
      "Loss: 1.0814756155014038\n",
      "Loss: 1.064276933670044\n",
      "Loss: 1.0900928974151611\n",
      "Loss: 1.0805583000183105\n",
      "Loss: 1.0478336811065674\n",
      "Loss: 1.0362319946289062\n",
      "Loss: 1.1100151538848877\n",
      "Loss: 1.0825631618499756\n",
      "Loss: 1.0535939931869507\n",
      "Loss: 1.088391900062561\n",
      "Loss: 1.0697721242904663\n",
      "Loss: 1.061965823173523\n",
      "Loss: 1.0919551849365234\n",
      "Loss: 1.0540305376052856\n",
      "Loss: 1.0987162590026855\n",
      "Loss: 1.0750266313552856\n",
      "Loss: 1.0345182418823242\n",
      "Loss: 1.0630738735198975\n",
      "Loss: 1.125289797782898\n",
      "Loss: 1.0375956296920776\n",
      "Loss: 1.1174442768096924\n",
      "Loss: 1.1081461906433105\n",
      "Loss: 1.1555712223052979\n",
      "Loss: 1.0816740989685059\n",
      "Loss: 1.0395439863204956\n",
      "Loss: 1.0276551246643066\n",
      "Loss: 1.106407642364502\n",
      "Loss: 1.0718146562576294\n",
      "Loss: 1.0550965070724487\n",
      "Loss: 1.0720402002334595\n",
      "Loss: 1.0136412382125854\n",
      "Loss: 1.0348042249679565\n",
      "Loss: 1.0489017963409424\n",
      "Loss: 0.9844323396682739\n",
      "Loss: 1.1378164291381836\n",
      "Loss: 1.1801691055297852\n",
      "Loss: 1.1434814929962158\n",
      "Loss: 1.127132773399353\n",
      "Loss: 1.0519726276397705\n",
      "Loss: 1.0166252851486206\n",
      "Loss: 1.0641798973083496\n",
      "Loss: 1.0619264841079712\n",
      "Loss: 1.0630557537078857\n",
      "Loss: 1.0554311275482178\n",
      "Loss: 1.1499449014663696\n",
      "Loss: 1.0467517375946045\n",
      "Loss: 1.0991109609603882\n",
      "Loss: 1.0802780389785767\n",
      "Loss: 1.0445029735565186\n",
      "Loss: 1.068113923072815\n",
      "Loss: 1.0280765295028687\n",
      "Loss: 1.0147979259490967\n",
      "Loss: 1.046391248703003\n",
      "Loss: 1.0754303932189941\n",
      "Loss: 1.1420400142669678\n",
      "Loss: 1.0642597675323486\n",
      "Loss: 1.0562427043914795\n",
      "Loss: 1.013849139213562\n",
      "Loss: 1.1175403594970703\n",
      "Loss: 1.0585150718688965\n",
      "Loss: 1.0536185503005981\n",
      "Loss: 1.0467807054519653\n",
      "Loss: 1.072057843208313\n",
      "Loss: 1.0424938201904297\n",
      "Loss: 1.1283636093139648\n",
      "Loss: 1.1077744960784912\n",
      "Loss: 1.017958402633667\n",
      "Loss: 1.1074955463409424\n",
      "Loss: 1.072327971458435\n",
      "Loss: 1.0250208377838135\n",
      "Loss: 1.062575101852417\n",
      "Loss: 1.1083571910858154\n",
      "Loss: 1.090482234954834\n",
      "Loss: 1.0171692371368408\n",
      "Loss: 1.099941611289978\n",
      "Loss: 1.0902025699615479\n",
      "Loss: 1.1349912881851196\n",
      "Loss: 1.1064512729644775\n",
      "Loss: 1.0889906883239746\n",
      "Loss: 0.9981583952903748\n",
      "Loss: 1.0285619497299194\n",
      "Loss: 1.1043397188186646\n",
      "Loss: 1.0431931018829346\n",
      "Loss: 1.026336431503296\n",
      "Loss: 1.1080219745635986\n",
      "Loss: 1.0224404335021973\n",
      "Loss: 1.0529580116271973\n",
      "Loss: 1.0601433515548706\n",
      "Loss: 1.0529992580413818\n",
      "Loss: 1.1016056537628174\n",
      "Loss: 1.012739896774292\n",
      "Loss: 1.0590126514434814\n",
      "Loss: 1.0783203840255737\n",
      "Loss: 1.242563009262085\n",
      "Loss: 1.0458457469940186\n",
      "Loss: 1.089389443397522\n",
      "Loss: 1.0277642011642456\n",
      "Loss: 1.0813381671905518\n",
      "Loss: 1.0642945766448975\n",
      "Loss: 0.9796804785728455\n",
      "Loss: 1.1512669324874878\n",
      "Loss: 1.0626555681228638\n",
      "Loss: 1.0682116746902466\n",
      "Loss: 1.122186303138733\n",
      "Loss: 1.0375871658325195\n",
      "Loss: 1.1046077013015747\n",
      "Loss: 1.0066624879837036\n",
      "Loss: 1.0951436758041382\n",
      "Loss: 1.0279029607772827\n",
      "Loss: 1.0250959396362305\n",
      "Loss: 1.1008373498916626\n",
      "Loss: 1.007590651512146\n",
      "Loss: 1.0792076587677002\n",
      "Loss: 1.1371808052062988\n",
      "Loss: 1.0066211223602295\n",
      "Loss: 1.0463157892227173\n",
      "Loss: 1.011064887046814\n",
      "Loss: 1.1174362897872925\n",
      "Loss: 1.0894654989242554\n",
      "Loss: 1.140032410621643\n",
      "Loss: 1.0637108087539673\n",
      "Loss: 1.1143327951431274\n",
      "Loss: 1.061368703842163\n",
      "Loss: 1.0105514526367188\n",
      "Loss: 1.1082067489624023\n",
      "Loss: 0.9858986735343933\n",
      "Loss: 1.1165283918380737\n",
      "Loss: 1.0990599393844604\n",
      "Loss: 1.1278355121612549\n",
      "Loss: 1.1189013719558716\n",
      "Loss: 0.978553295135498\n",
      "Loss: 1.1094627380371094\n",
      "Loss: 1.0151034593582153\n",
      "Loss: 1.0805238485336304\n",
      "Loss: 1.1349437236785889\n",
      "Loss: 1.0526108741760254\n",
      "Loss: 1.0982853174209595\n",
      "Loss: 1.054628849029541\n",
      "Loss: 1.0797045230865479\n",
      "Loss: 1.0518360137939453\n",
      "Loss: 1.107394814491272\n",
      "Loss: 1.061972975730896\n",
      "Loss: 1.0539443492889404\n",
      "Loss: 1.1094211339950562\n",
      "Loss: 1.0439027547836304\n",
      "Loss: 1.0350079536437988\n",
      "Loss: 1.0806082487106323\n",
      "Loss: 1.0860036611557007\n",
      "Loss: 1.036475419998169\n",
      "Loss: 1.0238810777664185\n",
      "Loss: 1.0298621654510498\n",
      "Loss: 1.0626479387283325\n",
      "Loss: 1.0732965469360352\n",
      "Loss: 1.1199105978012085\n",
      "Loss: 1.0320348739624023\n",
      "Loss: 1.0706183910369873\n",
      "Loss: 1.0612478256225586\n",
      "Loss: 1.0811923742294312\n",
      "Loss: 1.037432312965393\n",
      "Loss: 1.0045866966247559\n",
      "Loss: 1.0624994039535522\n",
      "Loss: 1.0932081937789917\n",
      "Loss: 1.0572346448898315\n",
      "Loss: 1.0536565780639648\n",
      "Loss: 1.0467675924301147\n",
      "Loss: 1.063178300857544\n",
      "Loss: 1.0799740552902222\n",
      "Loss: 1.046631097793579\n",
      "Loss: 1.1364808082580566\n",
      "Loss: 1.0987521409988403\n",
      "Loss: 1.0165667533874512\n",
      "Loss: 1.1366925239562988\n",
      "Loss: 1.0556259155273438\n",
      "Loss: 1.0535260438919067\n",
      "Loss: 1.1010395288467407\n",
      "Loss: 1.1118478775024414\n",
      "Loss: 1.0714601278305054\n",
      "Loss: 1.1383049488067627\n",
      "Loss: 1.0808064937591553\n",
      "-------- \n",
      " Epoch 10: Average Loss: 1.0705376863479614\n",
      "Epoch 12\n",
      "-----------------------\n",
      "Loss: 1.0162824392318726\n",
      "Loss: 1.0632336139678955\n",
      "Loss: 1.0812140703201294\n",
      "Loss: 1.1475757360458374\n",
      "Loss: 1.0300922393798828\n",
      "Loss: 0.9797999262809753\n",
      "Loss: 0.9949988126754761\n",
      "Loss: 1.0446828603744507\n",
      "Loss: 1.0444775819778442\n",
      "Loss: 1.074275255203247\n",
      "Loss: 1.0343812704086304\n",
      "Loss: 1.1264272928237915\n",
      "Loss: 1.0645426511764526\n",
      "Loss: 1.0343669652938843\n",
      "Loss: 1.0908454656600952\n",
      "Loss: 1.0303208827972412\n",
      "Loss: 1.0558067560195923\n",
      "Loss: 1.0742161273956299\n",
      "Loss: 1.0458755493164062\n",
      "Loss: 1.1334121227264404\n",
      "Loss: 1.0554783344268799\n",
      "Loss: 1.1103687286376953\n",
      "Loss: 1.1277645826339722\n",
      "Loss: 1.0198276042938232\n",
      "Loss: 1.0860379934310913\n",
      "Loss: 1.013237476348877\n",
      "Loss: 1.0630406141281128\n",
      "Loss: 1.0565170049667358\n",
      "Loss: 0.9754522442817688\n",
      "Loss: 1.0250720977783203\n",
      "Loss: 1.0733758211135864\n",
      "Loss: 1.1641212701797485\n",
      "Loss: 1.0635660886764526\n",
      "Loss: 1.0021194219589233\n",
      "Loss: 1.1563711166381836\n",
      "Loss: 1.1195422410964966\n",
      "Loss: 1.0638675689697266\n",
      "Loss: 1.007110357284546\n",
      "Loss: 1.1001205444335938\n",
      "Loss: 1.1120924949645996\n",
      "Loss: 1.0856311321258545\n",
      "Loss: 1.0443130731582642\n",
      "Loss: 1.1112066507339478\n",
      "Loss: 1.03097665309906\n",
      "Loss: 1.002240777015686\n",
      "Loss: 1.1070525646209717\n",
      "Loss: 1.071881651878357\n",
      "Loss: 1.1505886316299438\n",
      "Loss: 1.0635324716567993\n",
      "Loss: 1.026610016822815\n",
      "Loss: 1.0989693403244019\n",
      "Loss: 1.1040189266204834\n",
      "Loss: 1.0819268226623535\n",
      "Loss: 1.0153383016586304\n",
      "Loss: 0.9953821897506714\n",
      "Loss: 1.0076273679733276\n",
      "Loss: 1.0969606637954712\n",
      "Loss: 1.0810837745666504\n",
      "Loss: 1.108479380607605\n",
      "Loss: 1.0922619104385376\n",
      "Loss: 1.0536952018737793\n",
      "Loss: 0.9909951090812683\n",
      "Loss: 1.0173020362854004\n",
      "Loss: 1.1082465648651123\n",
      "Loss: 1.0543975830078125\n",
      "Loss: 1.0848201513290405\n",
      "Loss: 1.0893654823303223\n",
      "Loss: 1.0918889045715332\n",
      "Loss: 1.0921183824539185\n",
      "Loss: 1.1160225868225098\n",
      "Loss: 1.0491670370101929\n",
      "Loss: 1.083513617515564\n",
      "Loss: 1.0555001497268677\n",
      "Loss: 1.0518722534179688\n",
      "Loss: 1.0727185010910034\n",
      "Loss: 1.0454449653625488\n",
      "Loss: 1.1035699844360352\n",
      "Loss: 1.0202829837799072\n",
      "Loss: 0.954348623752594\n",
      "Loss: 1.1173186302185059\n",
      "Loss: 1.0723538398742676\n",
      "Loss: 1.0996992588043213\n",
      "Loss: 1.0066537857055664\n",
      "Loss: 0.9976175427436829\n",
      "Loss: 1.0137944221496582\n",
      "Loss: 1.0031754970550537\n",
      "Loss: 1.1428114175796509\n",
      "Loss: 1.1233868598937988\n",
      "Loss: 1.082861065864563\n",
      "Loss: 0.9784155488014221\n",
      "Loss: 1.0476622581481934\n",
      "Loss: 1.0416970252990723\n",
      "Loss: 1.0840034484863281\n",
      "Loss: 1.1070622205734253\n",
      "Loss: 1.0906318426132202\n",
      "Loss: 1.0302802324295044\n",
      "Loss: 1.0835541486740112\n",
      "Loss: 1.0458040237426758\n",
      "Loss: 1.1418615579605103\n",
      "Loss: 1.0547354221343994\n",
      "Loss: 1.0088120698928833\n",
      "Loss: 1.0543324947357178\n",
      "Loss: 1.0560848712921143\n",
      "Loss: 1.1183065176010132\n",
      "Loss: 0.9966071248054504\n",
      "Loss: 1.1272293329238892\n",
      "Loss: 1.053338646888733\n",
      "Loss: 1.1328260898590088\n",
      "Loss: 1.033423662185669\n",
      "Loss: 1.04237961769104\n",
      "Loss: 1.072108507156372\n",
      "Loss: 1.0541143417358398\n",
      "Loss: 1.0825750827789307\n",
      "Loss: 1.0545376539230347\n",
      "Loss: 1.1530596017837524\n",
      "Loss: 1.1486657857894897\n",
      "Loss: 1.053767204284668\n",
      "Loss: 1.105171799659729\n",
      "Loss: 1.0908048152923584\n",
      "Loss: 1.0877634286880493\n",
      "Loss: 1.0986374616622925\n",
      "Loss: 1.0820385217666626\n",
      "Loss: 1.0927560329437256\n",
      "Loss: 1.067481279373169\n",
      "Loss: 1.090569019317627\n",
      "Loss: 1.0604376792907715\n",
      "Loss: 1.104290246963501\n",
      "Loss: 1.0447871685028076\n",
      "Loss: 1.0917725563049316\n",
      "Loss: 1.0456770658493042\n",
      "Loss: 1.0378204584121704\n",
      "Loss: 1.0425549745559692\n",
      "Loss: 1.0290424823760986\n",
      "Loss: 1.1000314950942993\n",
      "Loss: 1.1038546562194824\n",
      "Loss: 1.0907599925994873\n",
      "Loss: 1.1380420923233032\n",
      "Loss: 1.1500154733657837\n",
      "Loss: 1.1687642335891724\n",
      "Loss: 1.088517665863037\n",
      "Loss: 1.1185202598571777\n",
      "Loss: 1.1182382106781006\n",
      "Loss: 1.04715895652771\n",
      "Loss: 1.0532596111297607\n",
      "Loss: 1.0729258060455322\n",
      "Loss: 1.0992906093597412\n",
      "Loss: 1.0294581651687622\n",
      "Loss: 1.0624113082885742\n",
      "Loss: 1.043882966041565\n",
      "Loss: 1.0234284400939941\n",
      "Loss: 1.0633177757263184\n",
      "Loss: 1.0637034177780151\n",
      "Loss: 1.0733833312988281\n",
      "Loss: 1.0242483615875244\n",
      "Loss: 1.1490217447280884\n",
      "Loss: 1.0725429058074951\n",
      "Loss: 1.0401489734649658\n",
      "Loss: 1.0540882349014282\n",
      "Loss: 1.063260793685913\n",
      "Loss: 1.0519293546676636\n",
      "Loss: 1.0406126976013184\n",
      "Loss: 1.0187840461730957\n",
      "Loss: 1.0326799154281616\n",
      "Loss: 1.0336155891418457\n",
      "Loss: 1.0633823871612549\n",
      "Loss: 1.0632452964782715\n",
      "Loss: 1.0961004495620728\n",
      "Loss: 1.0709962844848633\n",
      "Loss: 1.065070390701294\n",
      "Loss: 1.1054810285568237\n",
      "Loss: 1.0729421377182007\n",
      "Loss: 1.0979222059249878\n",
      "Loss: 1.0849145650863647\n",
      "Loss: 1.0485261678695679\n",
      "Loss: 1.064588189125061\n",
      "Loss: 1.0931684970855713\n",
      "Loss: 1.1554664373397827\n",
      "Loss: 1.0034111738204956\n",
      "Loss: 1.1105214357376099\n",
      "Loss: 1.0804097652435303\n",
      "Loss: 1.1157793998718262\n",
      "Loss: 1.0528442859649658\n",
      "Loss: 1.0629346370697021\n",
      "Loss: 1.146842122077942\n",
      "Loss: 1.0609835386276245\n",
      "Loss: 1.0718199014663696\n",
      "Loss: 1.0727972984313965\n",
      "Loss: 0.967464804649353\n",
      "Loss: 1.1256401538848877\n",
      "Loss: 1.0148431062698364\n",
      "Loss: 1.0123639106750488\n",
      "Loss: 1.0011085271835327\n",
      "Loss: 1.0428111553192139\n",
      "Loss: 1.0628900527954102\n",
      "Loss: 1.0247752666473389\n",
      "Loss: 1.0985747575759888\n",
      "Loss: 1.064896583557129\n",
      "Loss: 1.1070305109024048\n",
      "Loss: 1.0461821556091309\n",
      "Loss: 1.0546342134475708\n",
      "Loss: 1.1125900745391846\n",
      "Loss: 1.0944277048110962\n",
      "Loss: 1.0869803428649902\n",
      "Loss: 1.0724332332611084\n",
      "Loss: 1.0909228324890137\n",
      "Loss: 1.1433078050613403\n",
      "Loss: 1.0009146928787231\n",
      "Loss: 1.080680251121521\n",
      "Loss: 1.1586692333221436\n",
      "Loss: 1.0309158563613892\n",
      "Loss: 1.0537184476852417\n",
      "Loss: 1.0557782649993896\n",
      "Loss: 1.016708493232727\n",
      "Loss: 1.033653974533081\n",
      "Loss: 1.0810166597366333\n",
      "Loss: 1.1081639528274536\n",
      "Loss: 1.0900704860687256\n",
      "Loss: 1.0750555992126465\n",
      "Loss: 1.1128829717636108\n",
      "Loss: 1.0354924201965332\n",
      "Loss: 1.0737661123275757\n",
      "Loss: 1.0618125200271606\n",
      "Loss: 1.1503986120224\n",
      "Loss: 1.0720311403274536\n",
      "Loss: 1.0884449481964111\n",
      "Loss: 1.1350882053375244\n",
      "Loss: 1.082193374633789\n",
      "Loss: 0.977894127368927\n",
      "Loss: 1.110351800918579\n",
      "Loss: 1.0199769735336304\n",
      "Loss: 1.0628941059112549\n",
      "Loss: 1.0607355833053589\n",
      "Loss: 1.0968133211135864\n",
      "Loss: 1.0890100002288818\n",
      "Loss: 1.055745244026184\n",
      "Loss: 1.0352232456207275\n",
      "Loss: 1.0317763090133667\n",
      "Loss: 1.129469394683838\n",
      "Loss: 1.0382003784179688\n",
      "Loss: 1.1157867908477783\n",
      "Loss: 1.0434556007385254\n",
      "Loss: 1.071529507637024\n",
      "Loss: 1.0357074737548828\n",
      "Loss: 1.0300917625427246\n",
      "Loss: 1.0281902551651\n",
      "Loss: 1.0728983879089355\n",
      "-------- \n",
      " Epoch 11: Average Loss: 1.070492148399353\n",
      "Epoch 13\n",
      "-----------------------\n",
      "Loss: 1.0437848567962646\n",
      "Loss: 0.9990076422691345\n",
      "Loss: 1.01300048828125\n",
      "Loss: 0.9879873991012573\n",
      "Loss: 1.1284674406051636\n",
      "Loss: 1.096941590309143\n",
      "Loss: 1.0556011199951172\n",
      "Loss: 1.0639796257019043\n",
      "Loss: 1.1115825176239014\n",
      "Loss: 1.0627235174179077\n",
      "Loss: 1.0634325742721558\n",
      "Loss: 1.0244969129562378\n",
      "Loss: 1.099127173423767\n",
      "Loss: 1.0613881349563599\n",
      "Loss: 1.0700464248657227\n",
      "Loss: 1.066264271736145\n",
      "Loss: 1.0269287824630737\n",
      "Loss: 1.193151831626892\n",
      "Loss: 1.127007246017456\n",
      "Loss: 1.090399980545044\n",
      "Loss: 1.0189776420593262\n",
      "Loss: 1.0439873933792114\n",
      "Loss: 1.1495479345321655\n",
      "Loss: 0.9756346344947815\n",
      "Loss: 1.0698000192642212\n",
      "Loss: 1.0744251012802124\n",
      "Loss: 1.0837162733078003\n",
      "Loss: 1.1725322008132935\n",
      "Loss: 1.0391247272491455\n",
      "Loss: 1.092507004737854\n",
      "Loss: 1.0723036527633667\n",
      "Loss: 0.9949643611907959\n",
      "Loss: 0.9821351170539856\n",
      "Loss: 1.104714035987854\n",
      "Loss: 1.0645830631256104\n",
      "Loss: 1.1107808351516724\n",
      "Loss: 1.117706298828125\n",
      "Loss: 1.0097557306289673\n",
      "Loss: 1.156775951385498\n",
      "Loss: 1.0709681510925293\n",
      "Loss: 1.04389226436615\n",
      "Loss: 1.1112700700759888\n",
      "Loss: 1.1260250806808472\n",
      "Loss: 1.0495712757110596\n",
      "Loss: 1.083073616027832\n",
      "Loss: 1.0995084047317505\n",
      "Loss: 1.1247930526733398\n",
      "Loss: 1.1168501377105713\n",
      "Loss: 1.056395173072815\n",
      "Loss: 1.0200421810150146\n",
      "Loss: 1.0992064476013184\n",
      "Loss: 1.1222354173660278\n",
      "Loss: 1.0783064365386963\n",
      "Loss: 1.0355312824249268\n",
      "Loss: 1.081000566482544\n",
      "Loss: 1.1187278032302856\n",
      "Loss: 1.1063982248306274\n",
      "Loss: 1.034233808517456\n",
      "Loss: 1.002166509628296\n",
      "Loss: 1.0807968378067017\n",
      "Loss: 1.0358270406723022\n",
      "Loss: 1.1471930742263794\n",
      "Loss: 1.014222264289856\n",
      "Loss: 1.0575804710388184\n",
      "Loss: 1.0637949705123901\n",
      "Loss: 1.0352694988250732\n",
      "Loss: 1.0635865926742554\n",
      "Loss: 1.1084325313568115\n",
      "Loss: 1.0444121360778809\n",
      "Loss: 1.0811947584152222\n",
      "Loss: 1.115369200706482\n",
      "Loss: 1.1068130731582642\n",
      "Loss: 1.0352283716201782\n",
      "Loss: 1.0531060695648193\n",
      "Loss: 1.04390549659729\n",
      "Loss: 1.0886731147766113\n",
      "Loss: 1.0142393112182617\n",
      "Loss: 1.0527688264846802\n",
      "Loss: 1.0761375427246094\n",
      "Loss: 1.0747449398040771\n",
      "Loss: 1.173840880393982\n",
      "Loss: 1.097559928894043\n",
      "Loss: 1.0709902048110962\n",
      "Loss: 1.097900629043579\n",
      "Loss: 1.0477560758590698\n",
      "Loss: 1.1127887964248657\n",
      "Loss: 1.0527558326721191\n",
      "Loss: 1.073296070098877\n",
      "Loss: 1.0538277626037598\n",
      "Loss: 1.1080527305603027\n",
      "Loss: 1.0758261680603027\n",
      "Loss: 1.0163906812667847\n",
      "Loss: 1.0349576473236084\n",
      "Loss: 1.0630710124969482\n",
      "Loss: 1.06327486038208\n",
      "Loss: 1.0158053636550903\n",
      "Loss: 1.0809886455535889\n",
      "Loss: 1.0368305444717407\n",
      "Loss: 0.9971984624862671\n",
      "Loss: 1.0585293769836426\n",
      "Loss: 1.0637016296386719\n",
      "Loss: 0.954018771648407\n",
      "Loss: 1.0988765954971313\n",
      "Loss: 1.0711205005645752\n",
      "Loss: 1.0033597946166992\n",
      "Loss: 1.04523766040802\n",
      "Loss: 1.0449557304382324\n",
      "Loss: 1.0719025135040283\n",
      "Loss: 1.0885447263717651\n",
      "Loss: 1.0508919954299927\n",
      "Loss: 1.0631600618362427\n",
      "Loss: 1.0746835470199585\n",
      "Loss: 1.0946269035339355\n",
      "Loss: 1.0886436700820923\n",
      "Loss: 1.062146782875061\n",
      "Loss: 0.999062180519104\n",
      "Loss: 1.0428838729858398\n",
      "Loss: 1.0829663276672363\n",
      "Loss: 1.0920557975769043\n",
      "Loss: 1.052603840827942\n",
      "Loss: 1.06285560131073\n",
      "Loss: 1.0437369346618652\n",
      "Loss: 1.0537351369857788\n",
      "Loss: 1.1008960008621216\n",
      "Loss: 1.1274011135101318\n",
      "Loss: 0.9788986444473267\n",
      "Loss: 1.0916619300842285\n",
      "Loss: 1.0505748987197876\n",
      "Loss: 1.0438408851623535\n",
      "Loss: 1.0981292724609375\n",
      "Loss: 1.0737029314041138\n",
      "Loss: 1.0004454851150513\n",
      "Loss: 1.0316952466964722\n",
      "Loss: 1.0037329196929932\n",
      "Loss: 1.062222957611084\n",
      "Loss: 1.0903470516204834\n",
      "Loss: 1.0716456174850464\n",
      "Loss: 1.0802834033966064\n",
      "Loss: 1.0135165452957153\n",
      "Loss: 1.1454474925994873\n",
      "Loss: 1.0310964584350586\n",
      "Loss: 1.0741016864776611\n",
      "Loss: 1.0821452140808105\n",
      "Loss: 1.0733224153518677\n",
      "Loss: 1.0627436637878418\n",
      "Loss: 1.065453052520752\n",
      "Loss: 1.0812681913375854\n",
      "Loss: 1.008561134338379\n",
      "Loss: 1.0626553297042847\n",
      "Loss: 1.0722596645355225\n",
      "Loss: 1.1485981941223145\n",
      "Loss: 0.9857529997825623\n",
      "Loss: 1.062954068183899\n",
      "Loss: 1.0811753273010254\n",
      "Loss: 1.0445349216461182\n",
      "Loss: 1.10173761844635\n",
      "Loss: 1.1369191408157349\n",
      "Loss: 1.0538921356201172\n",
      "Loss: 1.0921459197998047\n",
      "Loss: 1.0465621948242188\n",
      "Loss: 1.0452547073364258\n",
      "Loss: 1.0330287218093872\n",
      "Loss: 1.0586439371109009\n",
      "Loss: 1.0611375570297241\n",
      "Loss: 1.0405559539794922\n",
      "Loss: 1.0431163311004639\n",
      "Loss: 1.060854434967041\n",
      "Loss: 1.1744691133499146\n",
      "Loss: 1.0520966053009033\n",
      "Loss: 1.036489486694336\n",
      "Loss: 1.0502127408981323\n",
      "Loss: 1.0808123350143433\n",
      "Loss: 1.1077306270599365\n",
      "Loss: 1.1097332239151\n",
      "Loss: 1.1840088367462158\n",
      "Loss: 0.9842873215675354\n",
      "Loss: 1.052533507347107\n",
      "Loss: 1.0620315074920654\n",
      "Loss: 1.0979747772216797\n",
      "Loss: 1.043778896331787\n",
      "Loss: 1.0595660209655762\n",
      "Loss: 1.0721430778503418\n",
      "Loss: 1.0291420221328735\n",
      "Loss: 1.1054201126098633\n",
      "Loss: 1.075457215309143\n",
      "Loss: 1.0270359516143799\n",
      "Loss: 0.9819779396057129\n",
      "Loss: 1.0165619850158691\n",
      "Loss: 1.1013647317886353\n",
      "Loss: 1.0826127529144287\n",
      "Loss: 1.0272881984710693\n",
      "Loss: 1.0530977249145508\n",
      "Loss: 1.1464331150054932\n",
      "Loss: 1.0525492429733276\n",
      "Loss: 1.0261554718017578\n",
      "Loss: 1.0725008249282837\n",
      "Loss: 1.0798929929733276\n",
      "Loss: 1.1003957986831665\n",
      "Loss: 1.0901975631713867\n",
      "Loss: 1.1126879453659058\n",
      "Loss: 0.9760270118713379\n",
      "Loss: 1.144424557685852\n",
      "Loss: 1.1005676984786987\n",
      "Loss: 1.0722832679748535\n",
      "Loss: 1.0992412567138672\n",
      "Loss: 1.0986407995224\n",
      "Loss: 1.0537525415420532\n",
      "Loss: 1.1157867908477783\n",
      "Loss: 1.061354160308838\n",
      "Loss: 1.0828614234924316\n",
      "Loss: 0.944812536239624\n",
      "Loss: 1.0445563793182373\n",
      "Loss: 1.1393921375274658\n",
      "Loss: 1.0530539751052856\n",
      "Loss: 1.080521583557129\n",
      "Loss: 1.0428085327148438\n",
      "Loss: 1.0534816980361938\n",
      "Loss: 1.0235835313796997\n",
      "Loss: 1.0428317785263062\n",
      "Loss: 1.1253811120986938\n",
      "Loss: 1.0730869770050049\n",
      "Loss: 1.0370478630065918\n",
      "Loss: 1.1190557479858398\n",
      "Loss: 1.0683612823486328\n",
      "Loss: 1.0620510578155518\n",
      "Loss: 1.1086506843566895\n",
      "Loss: 1.0012208223342896\n",
      "Loss: 0.993933379650116\n",
      "Loss: 1.1900944709777832\n",
      "Loss: 1.0733187198638916\n",
      "Loss: 1.1318050622940063\n",
      "Loss: 1.0267159938812256\n",
      "Loss: 1.080116868019104\n",
      "Loss: 1.0897889137268066\n",
      "Loss: 1.055076241493225\n",
      "Loss: 1.0376238822937012\n",
      "Loss: 1.0599943399429321\n",
      "Loss: 1.0253119468688965\n",
      "Loss: 1.0340092182159424\n",
      "Loss: 1.099897027015686\n",
      "Loss: 1.107431173324585\n",
      "Loss: 1.017428994178772\n",
      "Loss: 1.0526827573776245\n",
      "Loss: 1.0807249546051025\n",
      "Loss: 1.067099928855896\n",
      "Loss: 1.1093906164169312\n",
      "-------- \n",
      " Epoch 12: Average Loss: 1.0704503059387207\n",
      "Epoch 14\n",
      "-----------------------\n",
      "Loss: 1.0994946956634521\n",
      "Loss: 1.0995817184448242\n",
      "Loss: 1.0646445751190186\n",
      "Loss: 0.9887073636054993\n",
      "Loss: 1.0450091361999512\n",
      "Loss: 1.053231120109558\n",
      "Loss: 1.017741322517395\n",
      "Loss: 1.0534169673919678\n",
      "Loss: 1.0905933380126953\n",
      "Loss: 1.121899127960205\n",
      "Loss: 1.1383750438690186\n",
      "Loss: 1.109884262084961\n",
      "Loss: 1.0825144052505493\n",
      "Loss: 1.044370174407959\n",
      "Loss: 1.0406023263931274\n",
      "Loss: 1.078080177307129\n",
      "Loss: 1.071815848350525\n",
      "Loss: 1.0593831539154053\n",
      "Loss: 1.1366908550262451\n",
      "Loss: 1.090337872505188\n",
      "Loss: 1.0489684343338013\n",
      "Loss: 1.0712295770645142\n",
      "Loss: 1.0827147960662842\n",
      "Loss: 1.1017613410949707\n",
      "Loss: 1.1286125183105469\n",
      "Loss: 1.0350487232208252\n",
      "Loss: 1.0627387762069702\n",
      "Loss: 1.0459150075912476\n",
      "Loss: 1.0656901597976685\n",
      "Loss: 1.0166677236557007\n",
      "Loss: 1.0276117324829102\n",
      "Loss: 1.105081558227539\n",
      "Loss: 1.0353896617889404\n",
      "Loss: 1.088286280632019\n",
      "Loss: 1.098259449005127\n",
      "Loss: 1.0531221628189087\n",
      "Loss: 1.0348763465881348\n",
      "Loss: 1.1806573867797852\n",
      "Loss: 1.111242413520813\n",
      "Loss: 1.0256025791168213\n",
      "Loss: 1.0292284488677979\n",
      "Loss: 1.0067648887634277\n",
      "Loss: 1.0706204175949097\n",
      "Loss: 1.107324481010437\n",
      "Loss: 1.1008262634277344\n",
      "Loss: 1.0692780017852783\n",
      "Loss: 1.0883569717407227\n",
      "Loss: 1.0809800624847412\n",
      "Loss: 0.9988898634910583\n",
      "Loss: 1.1274127960205078\n",
      "Loss: 1.0811171531677246\n",
      "Loss: 1.1255780458450317\n",
      "Loss: 1.0997557640075684\n",
      "Loss: 1.1272966861724854\n",
      "Loss: 1.0772172212600708\n",
      "Loss: 1.0168415307998657\n",
      "Loss: 1.0275756120681763\n",
      "Loss: 1.1749844551086426\n",
      "Loss: 1.0994994640350342\n",
      "Loss: 1.0233731269836426\n",
      "Loss: 1.0528346300125122\n",
      "Loss: 1.0639021396636963\n",
      "Loss: 1.1540284156799316\n",
      "Loss: 1.1792750358581543\n",
      "Loss: 1.09259033203125\n",
      "Loss: 1.0547056198120117\n",
      "Loss: 1.063800573348999\n",
      "Loss: 1.0731643438339233\n",
      "Loss: 1.0536959171295166\n",
      "Loss: 1.1080563068389893\n",
      "Loss: 1.053794264793396\n",
      "Loss: 1.0151344537734985\n",
      "Loss: 1.1442956924438477\n",
      "Loss: 1.0769916772842407\n",
      "Loss: 1.119915246963501\n",
      "Loss: 1.08399498462677\n",
      "Loss: 1.063855528831482\n",
      "Loss: 1.0469896793365479\n",
      "Loss: 1.0822288990020752\n",
      "Loss: 1.069476842880249\n",
      "Loss: 1.0377850532531738\n",
      "Loss: 1.1175739765167236\n",
      "Loss: 1.072446346282959\n",
      "Loss: 1.0693644285202026\n",
      "Loss: 1.061179757118225\n",
      "Loss: 1.09251070022583\n",
      "Loss: 1.1049377918243408\n",
      "Loss: 1.0640215873718262\n",
      "Loss: 1.1010714769363403\n",
      "Loss: 1.1290860176086426\n",
      "Loss: 1.0720221996307373\n",
      "Loss: 1.0538924932479858\n",
      "Loss: 1.0522727966308594\n",
      "Loss: 1.0541797876358032\n",
      "Loss: 1.0000638961791992\n",
      "Loss: 1.1155734062194824\n",
      "Loss: 1.0184096097946167\n",
      "Loss: 1.0607140064239502\n",
      "Loss: 1.0615336894989014\n",
      "Loss: 1.025169849395752\n",
      "Loss: 1.0623475313186646\n",
      "Loss: 1.09215247631073\n",
      "Loss: 1.0896966457366943\n",
      "Loss: 1.0256949663162231\n",
      "Loss: 0.9925187230110168\n",
      "Loss: 1.0720018148422241\n",
      "Loss: 1.0903691053390503\n",
      "Loss: 1.0107312202453613\n",
      "Loss: 1.142859935760498\n",
      "Loss: 1.1556564569473267\n",
      "Loss: 1.0092105865478516\n",
      "Loss: 1.053328275680542\n",
      "Loss: 1.0379228591918945\n",
      "Loss: 1.130477786064148\n",
      "Loss: 1.081205129623413\n",
      "Loss: 1.0272244215011597\n",
      "Loss: 1.0994187593460083\n",
      "Loss: 1.0156724452972412\n",
      "Loss: 1.093946933746338\n",
      "Loss: 1.052878499031067\n",
      "Loss: 0.9912026524543762\n",
      "Loss: 1.0734477043151855\n",
      "Loss: 1.0725383758544922\n",
      "Loss: 1.1099305152893066\n",
      "Loss: 1.0482486486434937\n",
      "Loss: 1.0282844305038452\n",
      "Loss: 1.0893393754959106\n",
      "Loss: 1.0996687412261963\n",
      "Loss: 1.090614914894104\n",
      "Loss: 1.0442886352539062\n",
      "Loss: 1.0483125448226929\n",
      "Loss: 1.0726470947265625\n",
      "Loss: 1.1191179752349854\n",
      "Loss: 1.0903151035308838\n",
      "Loss: 1.0542250871658325\n",
      "Loss: 1.0855673551559448\n",
      "Loss: 1.1359180212020874\n",
      "Loss: 1.0254429578781128\n",
      "Loss: 1.0199047327041626\n",
      "Loss: 1.0270421504974365\n",
      "Loss: 1.0992724895477295\n",
      "Loss: 1.134164571762085\n",
      "Loss: 1.0629067420959473\n",
      "Loss: 1.1274399757385254\n",
      "Loss: 0.9710936546325684\n",
      "Loss: 1.0359156131744385\n",
      "Loss: 1.0616774559020996\n",
      "Loss: 1.1296720504760742\n",
      "Loss: 1.073015570640564\n",
      "Loss: 1.0936845541000366\n",
      "Loss: 1.0919677019119263\n",
      "Loss: 1.0546890497207642\n",
      "Loss: 1.1178261041641235\n",
      "Loss: 1.028497338294983\n",
      "Loss: 1.0888800621032715\n",
      "Loss: 1.035468578338623\n",
      "Loss: 0.9891260862350464\n",
      "Loss: 1.0813885927200317\n",
      "Loss: 1.0438839197158813\n",
      "Loss: 1.0622501373291016\n",
      "Loss: 0.9795023798942566\n",
      "Loss: 1.1490577459335327\n",
      "Loss: 1.0534377098083496\n",
      "Loss: 1.12774658203125\n",
      "Loss: 1.1202309131622314\n",
      "Loss: 1.071016788482666\n",
      "Loss: 1.0913399457931519\n",
      "Loss: 1.0837963819503784\n",
      "Loss: 1.0826034545898438\n",
      "Loss: 1.0250118970870972\n",
      "Loss: 1.105838418006897\n",
      "Loss: 0.9980257749557495\n",
      "Loss: 1.1288570165634155\n",
      "Loss: 1.1371030807495117\n",
      "Loss: 1.0723352432250977\n",
      "Loss: 1.0360833406448364\n",
      "Loss: 1.0887730121612549\n",
      "Loss: 1.045688271522522\n",
      "Loss: 1.07256019115448\n",
      "Loss: 1.0910791158676147\n",
      "Loss: 1.088426947593689\n",
      "Loss: 1.1212173700332642\n",
      "Loss: 1.0516142845153809\n",
      "Loss: 1.1175143718719482\n",
      "Loss: 1.0363242626190186\n",
      "Loss: 1.0458073616027832\n",
      "Loss: 1.0623195171356201\n",
      "Loss: 1.0624582767486572\n",
      "Loss: 1.063118577003479\n",
      "Loss: 1.1077172756195068\n",
      "Loss: 1.0623255968093872\n",
      "Loss: 0.9895264506340027\n",
      "Loss: 1.0016005039215088\n",
      "Loss: 1.1366794109344482\n",
      "Loss: 1.099311113357544\n",
      "Loss: 1.1187610626220703\n",
      "Loss: 1.054079294204712\n",
      "Loss: 1.0365543365478516\n",
      "Loss: 1.0345808267593384\n",
      "Loss: 1.0724623203277588\n",
      "Loss: 1.034698247909546\n",
      "Loss: 1.1191552877426147\n",
      "Loss: 1.0061137676239014\n",
      "Loss: 1.090311050415039\n",
      "Loss: 1.1091896295547485\n",
      "Loss: 1.1176433563232422\n",
      "Loss: 1.0807873010635376\n",
      "Loss: 1.098860740661621\n",
      "Loss: 1.1100395917892456\n",
      "Loss: 1.0913054943084717\n",
      "Loss: 0.9889230132102966\n",
      "Loss: 1.0621392726898193\n",
      "Loss: 1.0171784162521362\n",
      "Loss: 1.0637372732162476\n",
      "Loss: 1.1122398376464844\n",
      "Loss: 1.0612679719924927\n",
      "Loss: 1.0815309286117554\n",
      "Loss: 1.1086572408676147\n",
      "Loss: 1.0742741823196411\n",
      "Loss: 1.0695017576217651\n",
      "Loss: 1.0635141134262085\n",
      "Loss: 1.0913301706314087\n",
      "Loss: 1.0429900884628296\n",
      "Loss: 1.0449600219726562\n",
      "Loss: 1.107194423675537\n",
      "Loss: 1.0179579257965088\n",
      "Loss: 1.1111077070236206\n",
      "Loss: 1.0734424591064453\n",
      "Loss: 0.9730629920959473\n",
      "Loss: 1.1214600801467896\n",
      "Loss: 0.9674656987190247\n",
      "Loss: 1.0437343120574951\n",
      "Loss: 0.9989013075828552\n",
      "Loss: 1.0630521774291992\n",
      "Loss: 1.1366147994995117\n",
      "Loss: 1.1009403467178345\n",
      "Loss: 1.0627098083496094\n",
      "Loss: 1.0010530948638916\n",
      "Loss: 1.080448865890503\n",
      "Loss: 1.0254294872283936\n",
      "Loss: 1.0085819959640503\n",
      "Loss: 1.0425586700439453\n",
      "Loss: 1.0079904794692993\n",
      "Loss: 1.038884162902832\n",
      "Loss: 1.0260511636734009\n",
      "Loss: 1.067103624343872\n",
      "-------- \n",
      " Epoch 13: Average Loss: 1.0704213380813599\n",
      "Epoch 15\n",
      "-----------------------\n",
      "Loss: 1.0740947723388672\n",
      "Loss: 1.140249252319336\n",
      "Loss: 1.0473569631576538\n",
      "Loss: 1.0558404922485352\n",
      "Loss: 1.1319016218185425\n",
      "Loss: 1.062546730041504\n",
      "Loss: 1.052412986755371\n",
      "Loss: 1.11961030960083\n",
      "Loss: 0.9632559418678284\n",
      "Loss: 1.0615235567092896\n",
      "Loss: 1.1380912065505981\n",
      "Loss: 1.0538586378097534\n",
      "Loss: 1.0543086528778076\n",
      "Loss: 1.0094159841537476\n",
      "Loss: 0.9986637830734253\n",
      "Loss: 1.063119888305664\n",
      "Loss: 1.0457003116607666\n",
      "Loss: 1.0905897617340088\n",
      "Loss: 1.0725511312484741\n",
      "Loss: 1.0990231037139893\n",
      "Loss: 1.0161763429641724\n",
      "Loss: 1.0903822183609009\n",
      "Loss: 1.1103545427322388\n",
      "Loss: 1.0242819786071777\n",
      "Loss: 1.1082792282104492\n",
      "Loss: 1.0536997318267822\n",
      "Loss: 1.0439506769180298\n",
      "Loss: 0.950050950050354\n",
      "Loss: 1.0342358350753784\n",
      "Loss: 1.1287578344345093\n",
      "Loss: 1.0726606845855713\n",
      "Loss: 1.0346403121948242\n",
      "Loss: 1.091261863708496\n",
      "Loss: 1.0451580286026\n",
      "Loss: 1.0627899169921875\n",
      "Loss: 1.1090682744979858\n",
      "Loss: 1.1183797121047974\n",
      "Loss: 1.062490463256836\n",
      "Loss: 1.0637661218643188\n",
      "Loss: 1.0814635753631592\n",
      "Loss: 1.016379714012146\n",
      "Loss: 0.9981935024261475\n",
      "Loss: 1.0716345310211182\n",
      "Loss: 1.006447672843933\n",
      "Loss: 1.178130865097046\n",
      "Loss: 1.127595067024231\n",
      "Loss: 1.0917400121688843\n",
      "Loss: 1.1094768047332764\n",
      "Loss: 1.0240082740783691\n",
      "Loss: 1.110249638557434\n",
      "Loss: 1.0880835056304932\n",
      "Loss: 1.0224361419677734\n",
      "Loss: 1.060438632965088\n",
      "Loss: 1.1567107439041138\n",
      "Loss: 1.0427378416061401\n",
      "Loss: 1.0235555171966553\n",
      "Loss: 0.9924662113189697\n",
      "Loss: 1.1093168258666992\n",
      "Loss: 1.1104686260223389\n",
      "Loss: 1.0018004179000854\n",
      "Loss: 1.1420226097106934\n",
      "Loss: 1.0917268991470337\n",
      "Loss: 1.0512616634368896\n",
      "Loss: 1.0434821844100952\n",
      "Loss: 1.0778237581253052\n",
      "Loss: 1.1318340301513672\n",
      "Loss: 1.0252244472503662\n",
      "Loss: 1.0272756814956665\n",
      "Loss: 1.1273335218429565\n",
      "Loss: 1.047396183013916\n",
      "Loss: 1.074468731880188\n",
      "Loss: 1.0165038108825684\n",
      "Loss: 1.0145665407180786\n",
      "Loss: 1.0517936944961548\n",
      "Loss: 1.1007781028747559\n",
      "Loss: 1.035668969154358\n",
      "Loss: 1.007054090499878\n",
      "Loss: 1.0798563957214355\n",
      "Loss: 1.0557094812393188\n",
      "Loss: 1.1144880056381226\n",
      "Loss: 1.0908207893371582\n",
      "Loss: 1.0795965194702148\n",
      "Loss: 1.1078267097473145\n",
      "Loss: 1.0811134576797485\n",
      "Loss: 1.0733473300933838\n",
      "Loss: 1.0614840984344482\n",
      "Loss: 1.0621687173843384\n",
      "Loss: 1.0655996799468994\n",
      "Loss: 1.0662639141082764\n",
      "Loss: 1.0812902450561523\n",
      "Loss: 1.0272012948989868\n",
      "Loss: 1.135870099067688\n",
      "Loss: 1.0525134801864624\n",
      "Loss: 1.1402286291122437\n",
      "Loss: 1.1261988878250122\n",
      "Loss: 1.0166926383972168\n",
      "Loss: 1.0993735790252686\n",
      "Loss: 1.0470490455627441\n",
      "Loss: 1.0262141227722168\n",
      "Loss: 1.1541179418563843\n",
      "Loss: 1.1176564693450928\n",
      "Loss: 1.0989928245544434\n",
      "Loss: 1.0536444187164307\n",
      "Loss: 1.0431727170944214\n",
      "Loss: 1.0626580715179443\n",
      "Loss: 1.072312831878662\n",
      "Loss: 1.0108529329299927\n",
      "Loss: 1.0891131162643433\n",
      "Loss: 1.0614181756973267\n",
      "Loss: 1.1258262395858765\n",
      "Loss: 1.0641100406646729\n",
      "Loss: 1.0891035795211792\n",
      "Loss: 1.080522894859314\n",
      "Loss: 1.018755316734314\n",
      "Loss: 1.0630309581756592\n",
      "Loss: 1.1534194946289062\n",
      "Loss: 1.0629287958145142\n",
      "Loss: 1.1332542896270752\n",
      "Loss: 1.069693684577942\n",
      "Loss: 1.1223031282424927\n",
      "Loss: 1.0718786716461182\n",
      "Loss: 1.0342841148376465\n",
      "Loss: 1.0789705514907837\n",
      "Loss: 1.1092747449874878\n",
      "Loss: 1.0901739597320557\n",
      "Loss: 1.0462816953659058\n",
      "Loss: 1.0982463359832764\n",
      "Loss: 1.0780389308929443\n",
      "Loss: 1.0632559061050415\n",
      "Loss: 1.0461089611053467\n",
      "Loss: 1.0618109703063965\n",
      "Loss: 1.074068546295166\n",
      "Loss: 1.0627429485321045\n",
      "Loss: 1.0992423295974731\n",
      "Loss: 1.0651193857192993\n",
      "Loss: 1.0454673767089844\n",
      "Loss: 0.9992157220840454\n",
      "Loss: 1.0351356267929077\n",
      "Loss: 1.0625971555709839\n",
      "Loss: 1.1380681991577148\n",
      "Loss: 1.0362746715545654\n",
      "Loss: 1.016961932182312\n",
      "Loss: 1.098920226097107\n",
      "Loss: 1.0627555847167969\n",
      "Loss: 1.0827757120132446\n",
      "Loss: 1.0455323457717896\n",
      "Loss: 1.1437678337097168\n",
      "Loss: 1.1088312864303589\n",
      "Loss: 1.063524842262268\n",
      "Loss: 1.0524406433105469\n",
      "Loss: 1.0821847915649414\n",
      "Loss: 1.0691862106323242\n",
      "Loss: 1.0430338382720947\n",
      "Loss: 1.0750612020492554\n",
      "Loss: 1.0693349838256836\n",
      "Loss: 1.0793582201004028\n",
      "Loss: 1.0642976760864258\n",
      "Loss: 1.0618489980697632\n",
      "Loss: 1.1181693077087402\n",
      "Loss: 1.054050326347351\n",
      "Loss: 1.0592210292816162\n",
      "Loss: 1.0508204698562622\n",
      "Loss: 1.0736762285232544\n",
      "Loss: 1.0511103868484497\n",
      "Loss: 1.0064761638641357\n",
      "Loss: 1.053881287574768\n",
      "Loss: 1.0813920497894287\n",
      "Loss: 1.0168461799621582\n",
      "Loss: 1.0714640617370605\n",
      "Loss: 1.1092884540557861\n",
      "Loss: 1.1276873350143433\n",
      "Loss: 1.0090880393981934\n",
      "Loss: 1.0994473695755005\n",
      "Loss: 1.1259669065475464\n",
      "Loss: 1.1003497838974\n",
      "Loss: 1.0071332454681396\n",
      "Loss: 1.0915141105651855\n",
      "Loss: 1.0968225002288818\n",
      "Loss: 1.1174066066741943\n",
      "Loss: 1.0638507604599\n",
      "Loss: 1.1594390869140625\n",
      "Loss: 1.0825141668319702\n",
      "Loss: 1.0724347829818726\n",
      "Loss: 1.0499504804611206\n",
      "Loss: 1.0674524307250977\n",
      "Loss: 1.100375771522522\n",
      "Loss: 1.081658124923706\n",
      "Loss: 1.11747407913208\n",
      "Loss: 1.1005637645721436\n",
      "Loss: 1.0332739353179932\n",
      "Loss: 1.055523157119751\n",
      "Loss: 1.0188016891479492\n",
      "Loss: 1.0461885929107666\n",
      "Loss: 1.053133487701416\n",
      "Loss: 1.044683575630188\n",
      "Loss: 1.0800834894180298\n",
      "Loss: 1.0604866743087769\n",
      "Loss: 1.044498324394226\n",
      "Loss: 1.0458652973175049\n",
      "Loss: 1.1276463270187378\n",
      "Loss: 1.0161514282226562\n",
      "Loss: 1.000301480293274\n",
      "Loss: 1.0899922847747803\n",
      "Loss: 1.0435746908187866\n",
      "Loss: 1.0457799434661865\n",
      "Loss: 1.1185425519943237\n",
      "Loss: 1.0266685485839844\n",
      "Loss: 1.0360337495803833\n",
      "Loss: 1.0900354385375977\n",
      "Loss: 1.133765459060669\n",
      "Loss: 1.0414880514144897\n",
      "Loss: 1.090230941772461\n",
      "Loss: 0.9891447424888611\n",
      "Loss: 1.0648887157440186\n",
      "Loss: 1.0462384223937988\n",
      "Loss: 1.0465590953826904\n",
      "Loss: 1.063652515411377\n",
      "Loss: 1.045994520187378\n",
      "Loss: 1.0086369514465332\n",
      "Loss: 1.0823516845703125\n",
      "Loss: 1.0642240047454834\n",
      "Loss: 1.1204912662506104\n",
      "Loss: 1.1076140403747559\n",
      "Loss: 1.0821720361709595\n",
      "Loss: 1.0644456148147583\n",
      "Loss: 1.0830744504928589\n",
      "Loss: 1.072351336479187\n",
      "Loss: 1.052597165107727\n",
      "Loss: 1.0822991132736206\n",
      "Loss: 1.0812441110610962\n",
      "Loss: 1.0069345235824585\n",
      "Loss: 1.1067585945129395\n",
      "Loss: 1.0979783535003662\n",
      "Loss: 1.071380853652954\n",
      "Loss: 1.0359374284744263\n",
      "Loss: 1.089754343032837\n",
      "Loss: 1.0724507570266724\n",
      "Loss: 1.1010921001434326\n",
      "Loss: 1.0633445978164673\n",
      "Loss: 1.0457720756530762\n",
      "Loss: 1.1098101139068604\n",
      "Loss: 1.0624840259552002\n",
      "Loss: 0.9987851977348328\n",
      "Loss: 1.0989232063293457\n",
      "Loss: 1.0515000820159912\n",
      "Loss: 0.9980365037918091\n",
      "-------- \n",
      " Epoch 14: Average Loss: 1.0703803300857544\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "train_losses = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3  # Number of epochs with no improvement after which to stop\n",
    "best_val_loss = float('inf')\n",
    "counter = 0  # Counter for tracking epochs without improvement\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"Epoch {epoch+1}\\n-----------------------\")\n",
    "\n",
    "    for i, batch in enumerate(train_set):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "             print(\"Loss: {}\".format(loss))\n",
    "             \n",
    "\n",
    "    average_loss = total_loss / len(train_set)\n",
    "    print(\"-\"*8,\"\\n\",\"Epoch {}: Average Loss: {}\".format(epoch, average_loss))\n",
    "\n",
    "    # Early stopping check    \n",
    "    if average_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), \"/home/labsilver/Moy/models/dnabert.pth\")\n",
    "        best_val_loss = average_loss\n",
    "        counter = 0  # Reset counter if there's improvement\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break  # Stop training\n",
    "    \n",
    "    train_losses.append(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302fff2b306c4bc380449d51d0b05fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.32725330508550227}\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(len(test_set)))\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "#roc_auc = evaluate.load(\"roc_auc\", \"multilabel\")\n",
    "model.eval()\n",
    "for batch in test_set:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    #roc_auc.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    progress_bar.update(1)\n",
    "print(metric.compute())#, roc_auc.compute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.5927, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(0.5878, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(0.5876, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(0.5874, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(0.5876, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(0.5875, device='cuda:0', grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_losses = []\n",
    "for loss in train_losses:\n",
    "    new_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = [epoch+1 for epoch in range(epoch)]\n",
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='epoch', ylabel='loss'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    'epoch': epoch,\n",
    "    'loss': new_losses\n",
    "}\n",
    "\n",
    "sns.lineplot(data=data, x='epoch', y='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3279903391006284,\n",
       " 0.32815024891863215,\n",
       " 0.32814183261242147,\n",
       " 0.32815305435403574,\n",
       " 0.3281474434832286,\n",
       " 0.32814183261242147]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='epoch', ylabel='accuracy'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ac_data = {\n",
    "    'epoch': epoch,\n",
    "    'accuracy': train_accuracies\n",
    "}\n",
    "\n",
    "sns.lineplot(data=ac_data, x='epoch', y='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.5}\n",
       "\n",
       "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
       "        >>> print(results)\n",
       "        {'accuracy': 3.0}\n",
       "\n",
       "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.8778625954198473}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2765, -0.3196, -0.0059],\n",
       "        [ 0.2765, -0.3196, -0.0059],\n",
       "        [ 0.2765, -0.3196, -0.0059],\n",
       "        [ 0.2765, -0.3196, -0.0059],\n",
       "        [ 0.2765, -0.3196, -0.0059],\n",
       "        [ 0.2765, -0.3196, -0.0059],\n",
       "        [ 0.2765, -0.3196, -0.0059],\n",
       "        [ 0.2765, -0.3196, -0.0059],\n",
       "        [ 0.2765, -0.3196, -0.0059]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = model(**batch)\n",
    "outputs.logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RuledDeepWAS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
